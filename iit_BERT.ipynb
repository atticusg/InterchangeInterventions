{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb85aad4",
   "metadata": {},
   "source": [
    "# Debiasing BERT on Sentiment Analysis across Gender\n",
    "\n",
    "This notebook works through debiasing a BERT model that is biased across gender on a sentiment analysis task. The notebook proceeds as follows:\n",
    "\n",
    "1. Train a BERT model (blackbox model) on the [SemEval 2018](https://competitions.codalab.org/competitions/17751) sentiment analysis task.\n",
    "2. Evaluate the blackbox model's bias across gender using the [EEC benchmark](https://saifmohammad.com/WebPages/Biases-SA.html). \n",
    "3. Learn a distributed alignment using DAS between the blackbox model and gender, using counterfactuals from the EEC benchmark dataset.\n",
    "4. Use the learned alignment to intervene on the blackbox model during inference time with a static \"gender\" representation vector (e.g. average of male representation, average of female representation, average of both, or null vector), and evaluate this debiased model on: (a) the original SemEval task, and (b) the new model's bias on EEC.\n",
    "\n",
    "Using distributed alignment search, we're able to intervene on a pre-trained BERT model in order to achieve a debiased model with similar accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5931d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import utils\n",
    "from trainer import BERTLIMTrainer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from LIM_bert import LIMBERTClassifier\n",
    "import dataset_nli\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ab78727",
   "metadata": {},
   "source": [
    "## Train Blackbox Model\n",
    "\n",
    "In this section, we train fine-tune a BERT model on the [SemEval 2018](https://competitions.codalab.org/competitions/17751) task. The task is to predict the joy sentiment of tweets from Twitter, provided human-generated labels in [0, 1]. The SemEval task also consists of detecting anger, sadness, and fear aspects. \n",
    "\n",
    "The original benchmark compares models using the pearson correlation coefficient. A coefficient above 68% places us within the top 20 models evaluated on the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce703fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['development', 'test-gold', 'training']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE = '../../local/SemEval2018-Task1-all-data/English/'\n",
    "task = 'EI'\n",
    "task_type = 'reg'\n",
    "path = os.path.join(BASE, f'{task}-{task_type}')\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d32e2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1616, 2), (290, 2), (1105, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_ei(path):\n",
    "    train_df = pd.read_csv(os.path.join(path, 'training', f'EI-{task_type}-En-joy-train.txt'), delimiter='\\t')\n",
    "    eval_df = pd.read_csv(os.path.join(path, 'development', f'2018-EI-{task_type}-En-joy-dev.txt'), delimiter='\\t')\n",
    "    test_df = pd.read_csv(os.path.join(path, 'test-gold', f'2018-EI-{task_type}-En-joy-test-gold.txt'), delimiter='\\t')\n",
    "\n",
    "    if 'Intensity Class' in train_df.columns:\n",
    "        classes = train_df['Intensity Class'].unique()\n",
    "        class_to_label = dict(zip(classes, range(len(classes))))\n",
    "        train_df['labels'] = train_df['Intensity Class'].map(class_to_label)\n",
    "        eval_df['labels'] = eval_df['Intensity Class'].map(class_to_label)\n",
    "        test_df['labels'] = test_df['Intensity Class'].map(class_to_label)\n",
    "    else:\n",
    "        train_df['labels'] = train_df['Intensity Score'].copy()\n",
    "        eval_df['labels'] = eval_df['Intensity Score'].copy()\n",
    "        test_df['labels'] = test_df['Intensity Score'].copy()\n",
    "\n",
    "    rename = {\n",
    "        'Tweet': 'text'\n",
    "    }\n",
    "    train_df = train_df[['Tweet', 'labels']].rename(columns=rename)\n",
    "    eval_df = eval_df[['Tweet', 'labels']].rename(columns=rename)\n",
    "    test_df = test_df[['Tweet', 'labels']].rename(columns=rename)\n",
    "\n",
    "    return train_df, eval_df, test_df\n",
    "\n",
    "def load_data_ec(path):\n",
    "    train_df = pd.read_csv(os.path.join(path, '2018-E-c-En-train.txt'), delimiter='\\t')\n",
    "    eval_df = pd.read_csv(os.path.join(path, '2018-E-c-En-dev.txt'), delimiter='\\t')\n",
    "    test_df = pd.read_csv(os.path.join(path, '2018-E-c-En-test-gold.txt'), delimiter='\\t')\n",
    "\n",
    "    \n",
    "    train_df['labels'] = train_df.apply(lambda r: r.iloc[2:].tolist(), axis=1)\n",
    "    eval_df['labels'] = eval_df.apply(lambda r: r.iloc[2:].tolist(), axis=1)\n",
    "    test_df['labels'] = test_df.apply(lambda r: r.iloc[2:].tolist(), axis=1)\n",
    "\n",
    "    rename = {\n",
    "        'Tweet': 'text'\n",
    "    }\n",
    "    train_df = train_df[['Tweet', 'labels']].rename(columns=rename)\n",
    "    eval_df = eval_df[['Tweet', 'labels']].rename(columns=rename)\n",
    "    test_df = test_df[['Tweet', 'labels']].rename(columns=rename)\n",
    "\n",
    "    return train_df, eval_df, test_df\n",
    "\n",
    "train_df, eval_df, test_df = load_data_ei(path)\n",
    "train_df.shape, eval_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cc550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semeval_dataset(df, tokenizer_name, data_size):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        # if X[0][-1] != \".\":\n",
    "        #     input = [\". \".join(X)]\n",
    "        # else:\n",
    "        #     input = [\" \".join(X)]\n",
    "        input = [X]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "            input,\n",
    "            max_length=128,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        if i == data_size:\n",
    "            break\n",
    "        x_base, x_mask = encoding(row['text'])\n",
    "        y_base = row['labels']\n",
    "        data.append((x_base, x_mask, y_base))\n",
    "    \n",
    "    base, base_mask, y = zip(*data)\n",
    "    X_base = (base, base_mask) \n",
    "    y_base = torch.tensor(y)\n",
    "\n",
    "    return X_base, y_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bde6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ii_benchmark import IIBenchmarkMoNli\n",
    "\n",
    "VAR = 0\n",
    "\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "data_size = train_df.shape[0]\n",
    "test_data_size = test_df.shape[0] - 1 # size is unfortunately 1 mod 8, and I'm too lazy to edit prediction code\n",
    "device = 'cuda:0'\n",
    "seed = 42\n",
    "\n",
    "iit_layer = 10\n",
    "hidden_dim_per_concept = 256\n",
    "\n",
    "intervention_ids_to_coords = {\n",
    "    VAR: [{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "}\n",
    "\n",
    "benchmark = IIBenchmarkMoNli(\n",
    "    variable_names=['GEN'],\n",
    "    data_parameters={\n",
    "        'train_size': data_size, 'test_size': test_data_size\n",
    "    },\n",
    "    model_parameters={\n",
    "        'weights_name': 'bert-base-uncased',\n",
    "        'max_length': 128,\n",
    "        'n_classes': 1,\n",
    "        'hidden_dim': 768,\n",
    "        'target_layers' : [iit_layer],\n",
    "        'target_dims':{\n",
    "            \"start\" : 0,\n",
    "            \"end\" : 786,\n",
    "        },\n",
    "        'debug':False, \n",
    "        'device': device,\n",
    "        'static_search': False,\n",
    "        'nested_disentangle_inplace': False\n",
    "    },\n",
    "    training_parameters={\n",
    "        'warm_start': False, 'max_iter': 200, 'batch_size': 8, 'n_iter_no_change': 10, \n",
    "        'shuffle_train': False, 'eta': 2e-5, 'device': device, 'save_checkpoint_per_epoch': True\n",
    "    },\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7e3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "LIM_bert = benchmark.create_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c36bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "LIM_trainer.model.set_analysis_mode(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c30c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_semeval_train, y_semeval_train = get_semeval_dataset(train_df, 'bert-base-uncased', data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d075dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 58. Training loss did not improve more than tol=1e-05. Final error is 0.9914800344849937."
     ]
    }
   ],
   "source": [
    "# _ = LIM_trainer.fit(\n",
    "#     X_semeval_train,\n",
    "#     y_semeval_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a06c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(LIM_trainer.model.state_dict(), 'semeval-blackbox-layer-10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2973ecd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'saved_models/blackbox-45-42.bin'\n",
    "\n",
    "LIM_trainer.model.load_state_dict(state_dict=torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64aec38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6917114316441573, pvalue=5.803894323100251e-158)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict(\n",
    "    X_semeval_test\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcad9012",
   "metadata": {},
   "source": [
    "## Evaluate Bias on EEC\n",
    "\n",
    "This section replicates the bias evaluation conducted in [Kiritchenko and Mohammad, 2018](https://saifmohammad.com/WebPages/Biases-SA.html). The Equity Evaluation Corpus (EEC) consists of pairs of sentences where the key gender noun is toggled between a stereotypically female and a stereotypically male instance (e.g. \"he feels happy\" vs. \"she feels happy\", \"the conversation with my son was shocking\" vs. \"the conversation with my daughter was shocking\"). \n",
    "\n",
    "To detect bias, the authors compare the model's predictions across the 429 pairs generated in the EEC dataset. Notably, the EEC dataset *does not have labels*, but instead evaluates equity by comparing the model's prediction on an input to its prediction on the corresponding (counterfactual) input where the gender is flipped. We can compare the following two measures in order to determine the bias in our model:\n",
    "1. Rate at which the model predicts one gender over another (%M > F and %F > M). \n",
    "2. Difference between the average model predictions (M - F and F - M), with signficance measured by a t-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e20a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEC_PATH = 'eec_blackbox.csv'\n",
    "\n",
    "class EEC_Dataset:\n",
    "    def __init__(self, embed_func, emotions=['joy']) -> None:\n",
    "        self.eec_df = pd.read_csv(EEC_PATH)\n",
    "        self.embed_func = embed_func\n",
    "        self.emotions = emotions\n",
    "\n",
    "        self.eec_df = self.eec_df[(self.eec_df['Emotion'].isin(emotions)) | (self.eec_df['Emotion'].isna())]\n",
    "\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        self.templates = self.eec_df['Template'].unique()\n",
    "        self.emotion_situation_words = self.eec_df[\n",
    "            self.eec_df['Template'].apply(lambda t: '<emotional situation word>' in t)\n",
    "        ]['Emotion word'].unique()\n",
    "        self.emotion_words = self.eec_df[\n",
    "            self.eec_df['Template'].apply(lambda t: '<emotion word>' in t)\n",
    "        ]['Emotion word'].unique()\n",
    "\n",
    "        nouns = self.eec_df['Person'].unique()\n",
    "        pro_nouns = [p for p in nouns if p[0].islower()]\n",
    "        offset = (len(pro_nouns) - 2) // 2\n",
    "        pronoun_matching = {\n",
    "            pro_nouns[i]: pro_nouns[i + offset] for i in range(offset)\n",
    "        }\n",
    "        pronoun_matching.update({ 'him': 'her' })\n",
    "        self.pronoun_matching = pronoun_matching\n",
    "\n",
    "        proper_nouns = {\n",
    "            'male': [p for p in self.eec_df[self.eec_df['Gender'] == 'male']['Person'].unique() if p[0].isupper()],\n",
    "            'female': [p for p in self.eec_df[self.eec_df['Gender'] == 'female']['Person'].unique() if p[0].isupper()]\n",
    "        }\n",
    "        self.proper_nouns = proper_nouns\n",
    "\n",
    "    def _get_pairs_dataset(self):\n",
    "        pairs_data = []\n",
    "        columns = ['Sentence_M', 'Sentence_F', 'Template', 'Emotion', 'Emotion Word', 'Person']\n",
    "        for template in self.templates:\n",
    "            if '<emotional situation word>' in template:\n",
    "                emotion_list = self.emotion_situation_words\n",
    "            elif '<emotion word>' in template:\n",
    "                emotion_list = self.emotion_words\n",
    "            else:\n",
    "                emotion_list = [None]\n",
    "\n",
    "            for w in emotion_list:\n",
    "                if w is None:\n",
    "                    state_df = self.eec_df[(self.eec_df['Template'] == template)]\n",
    "                else:\n",
    "                    state_df = self.eec_df[(self.eec_df['Emotion word'] == w) & (self.eec_df['Template'] == template)]\n",
    "\n",
    "                for base_pronoun, flip_pronoun in self.pronoun_matching.items():\n",
    "                    if (base_pronoun == 'he' and '<person subject>' not in template) or \\\n",
    "                        (base_pronoun == 'him' and '<person object>' not in template):\n",
    "                        continue\n",
    "                    \n",
    "                    # ensure that we only have one of each noun\n",
    "                    assert(state_df[state_df['Person'] == base_pronoun].shape[0] == 1)\n",
    "\n",
    "                    pairs_data.append([\n",
    "                        state_df[state_df['Person'] == base_pronoun].iloc[0]['Sentence'],\n",
    "                        state_df[state_df['Person'] == flip_pronoun].iloc[0]['Sentence'],\n",
    "                        template,\n",
    "                        state_df[state_df['Person'] == flip_pronoun].iloc[0]['Emotion'],\n",
    "                        w, \n",
    "                        'pronoun'\n",
    "                    ])\n",
    "\n",
    "                pairs_data.append([\n",
    "                    [\n",
    "                        state_df[state_df['Person'] == male_proper_noun].iloc[0]['Sentence']\n",
    "                        for male_proper_noun in self.proper_nouns['male']\n",
    "                    ],\n",
    "                    [\n",
    "                        state_df[state_df['Person'] == female_proper_noun].iloc[0]['Sentence']\n",
    "                        for female_proper_noun in self.proper_nouns['female']\n",
    "                    ],\n",
    "                    template,\n",
    "                    state_df.iloc[0]['Emotion'],\n",
    "                    w,\n",
    "                    'proper noun'\n",
    "                ])\n",
    "\n",
    "        pairs_df = pd.DataFrame(pairs_data, columns=columns)\n",
    "        return pairs_df\n",
    "\n",
    "    def get_dataset(self, variable):\n",
    "        pairs_df = self._get_pairs_dataset()\n",
    "        pronoun_data = []\n",
    "        proper_noun_data = []\n",
    "        for i, row in pairs_df.iterrows():\n",
    "            if row['Person'] == 'pronoun':\n",
    "                if variable == 'male':\n",
    "                    base_x, base_mask = self.embed_func(row['Sentence_M'])\n",
    "                else:\n",
    "                    base_x, base_mask = self.embed_func(row['Sentence_F'])\n",
    "                pronoun_data.append((base_x, base_mask))\n",
    "            else:\n",
    "                if variable == 'male':\n",
    "                    embedded = [self.embed_func(s) for s in row['Sentence_M']]\n",
    "                else:\n",
    "                    embedded = [self.embed_func(s) for s in row['Sentence_F']]\n",
    "                base_x, base_mask = zip(*embedded)\n",
    "                proper_noun_data.append((base_x, base_mask))\n",
    "        \n",
    "        base_pronoun, base_mask_pronoun = zip(*pronoun_data)\n",
    "        self.base_pronoun = base_pronoun\n",
    "        self.base_mask_pronoun = base_mask_pronoun\n",
    "\n",
    "        base_proper_noun, base_mask_proper_noun = zip(*proper_noun_data)\n",
    "        self.base_proper_noun = base_proper_noun\n",
    "        self.base_mask_proper_noun = base_mask_proper_noun\n",
    "        return (self.base_pronoun, self.base_mask_pronoun), (self.base_proper_noun, self.base_mask_proper_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25099e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EEC_dataset(tokenizer_name):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    def encoding(X):\n",
    "        input = [X]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    eec_dataset = EEC_Dataset(embed_func=encoding)\n",
    "\n",
    "    pronoun_dataset_m, proper_noun_dataset_m = eec_dataset.get_dataset('male')\n",
    "    pronoun_dataset_f, proper_noun_dataset_f = eec_dataset.get_dataset('female')\n",
    "\n",
    "    return pronoun_dataset_m, proper_noun_dataset_m, pronoun_dataset_f, proper_noun_dataset_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a62799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_EEC(LIM_trainer, dataset):\n",
    "    pronoun_dataset_m, proper_noun_dataset_m, pronoun_dataset_f, proper_noun_dataset_f = dataset\n",
    "\n",
    "    predictions_m = LIM_trainer.predict(pronoun_dataset_m)\n",
    "    predictions_f = LIM_trainer.predict(pronoun_dataset_f)\n",
    "\n",
    "    for x_m in zip(*proper_noun_dataset_m):\n",
    "        predictions_m = torch.cat((\n",
    "            predictions_m,\n",
    "            LIM_trainer.predict(x_m).mean().unsqueeze(0)\n",
    "        ))\n",
    "    \n",
    "    for x_f in zip(*proper_noun_dataset_f):\n",
    "        predictions_f = torch.cat((\n",
    "            predictions_f,\n",
    "            LIM_trainer.predict(x_f).mean().unsqueeze(0)\n",
    "        ))\n",
    "    \n",
    "    return predictions_m, predictions_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6171d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eec_dataset = get_EEC_dataset('bert-base-uncased')\n",
    "predictions_m, predictions_f = evaluate_on_EEC(LIM_trainer, eec_dataset)\n",
    "\n",
    "predictions_m = predictions_m.detach().cpu().numpy()\n",
    "predictions_f = predictions_f.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e6e8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5847737 0.54655355\n",
      "0.038220167\n"
     ]
    }
   ],
   "source": [
    "print(predictions_m.mean(), predictions_f.mean())\n",
    "print(predictions_m.mean() - predictions_f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a84046ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9044289044289044\n"
     ]
    }
   ],
   "source": [
    "print((predictions_m > predictions_f).sum() / len(predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "737020da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=5.002500184837189, pvalue=6.868004333009701e-07)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(predictions_m, predictions_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6d096b0",
   "metadata": {},
   "source": [
    "The code below saves the blackbox model predictions as static labels for the interchange intervention training in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b77d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EEC_sentence_inputs(tokenizer_name):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    def encoding(X):\n",
    "        input = [X]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    eec_df = pd.read_csv(EEC_PATH)\n",
    "\n",
    "    data = []\n",
    "    for sentence in eec_df['Sentence'].values:\n",
    "        data.append(encoding(sentence))\n",
    "    \n",
    "    inputs, masks = zip(*data)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d8fcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_eec = get_EEC_sentence_inputs('bert-base-uncased')\n",
    "\n",
    "# blackbox_preds = LIM_trainer.predict(X_eec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae3db084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEC_PATH = 'eec_blackbox.csv'\n",
    "\n",
    "# eec_df = pd.read_csv(EEC_PATH)\n",
    "\n",
    "# eec_df['Prediction'] = blackbox_preds.detach().cpu().numpy()\n",
    "\n",
    "# eec_df.to_csv(EEC_PATH[:-4] + '-layer-10-iter-45-seed-42.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "851a9dba",
   "metadata": {},
   "source": [
    "## Learning Distributed Alignments on EEC\n",
    "\n",
    "In this section, we learn an alignment between the sensitive attribute in our causal graph (gender) and the biased blackbox model. We use the EEC dataset to generate our counterfactuals, and we use *the original model's output* as our target behavior. \n",
    "\n",
    "Note that in this section, we are not trying to debias our model yet. In fact, we are trying to completely replicate the blackbox model's behavior, including its initial bias. But the key idea is that we hope to do so in an interpretable, *intervenable* way that will allow us to later debias the very same model.\n",
    "\n",
    "Small note: when using analysis mode set to `True`, the code below runs Distributed Alignment Search (DAS). When analysis mode is set to `False`, the code trains a Causal Proxy Model (CPM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43322a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: this path should correspond to the saved outputs of the model we want to debias\n",
    "EEC_PATH = 'eec_blackbox-layer-10-iter-45-seed-42.csv' \n",
    "\n",
    "CONTROLS = [] # ['Emotion word'] # ['Emotion word', 'Template']\n",
    "SEED = 42\n",
    "VAR = 0\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "class IIT_EEC_Dataset:\n",
    "    def __init__(self, embed_func, split, size, emotion='joy'):\n",
    "        self.embed_func = embed_func\n",
    "        self.split = split\n",
    "        self.size = size\n",
    "        eec_df = pd.read_csv(EEC_PATH)\n",
    "        eec_df = eec_df[eec_df['Emotion'] == emotion].reset_index(drop=True)\n",
    "        self.train_df, self.test_df = train_test_split(eec_df, train_size=TRAIN_SIZE, shuffle=True)\n",
    "        self.eec_df = self.train_df if self.split == 'train' else self.test_df\n",
    "        self.variables = list(self.eec_df['Gender'].unique())\n",
    "        self.controls = CONTROLS\n",
    "        assert len(self.variables) == 2\n",
    "        assert self.size % 2 == 0\n",
    "\n",
    "    def flip(self, v):\n",
    "        if v == self.variables[0]:\n",
    "            return self.variables[1]\n",
    "        else:\n",
    "            return self.variables[0]\n",
    "\n",
    "    def get_intervention(self, base, source):\n",
    "        return VAR\n",
    "\n",
    "    def create_dataset_by_variable_states(self):\n",
    "        data = {}\n",
    "        variables = self.controls + ['Gender']\n",
    "        control_values = [list(self.eec_df[v].unique()) for v in variables]\n",
    "        num_classes = reduce(lambda a, b: a * b, [len(l) for l in control_values])\n",
    "        s = self.size // num_classes\n",
    "        for variable_state in itertools.product(*control_values):\n",
    "            state_data = self.eec_df[\n",
    "                reduce(\n",
    "                    lambda a, b: a & b,\n",
    "                    [self.eec_df[variables[i]] == variable_state[i] for i in range(len(variables))]\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # for now, simply ignore missing data\n",
    "            if state_data.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            state_data = state_data.sample(s, random_state=SEED)\n",
    "\n",
    "            data[variable_state] = []\n",
    "            for i, r in state_data.iterrows():\n",
    "                base_x, base_mask = self.embed_func(r['Sentence'])\n",
    "                base_label = r['Prediction']\n",
    "                data[variable_state].append((base_x, base_mask, base_label))\n",
    "        return data\n",
    "\n",
    "    def create_dataset(self):\n",
    "        data = []\n",
    "        variables = self.controls + ['Gender']\n",
    "        control_values = [list(self.eec_df[v].unique()) for v in variables]\n",
    "        num_classes = reduce(lambda a, b: a * b, [len(l) for l in control_values])\n",
    "        s = self.size // num_classes # number of pairs per variable state\n",
    "        for base_state in itertools.product(*control_values):\n",
    "            base_df = self.eec_df[\n",
    "                reduce(\n",
    "                    lambda a, b: a & b,\n",
    "                    [self.eec_df[variables[i]] == base_state[i] for i in range(len(variables))]\n",
    "                )\n",
    "            ].copy().reset_index(drop=True)\n",
    "\n",
    "            # keep all controls the same, but flip gender value\n",
    "            source_state = list(base_state)[:-1] + [self.flip(base_state[-1])]\n",
    "            source_df = self.eec_df[\n",
    "                reduce(\n",
    "                    lambda a, b: a & b,\n",
    "                    [self.eec_df[variables[i]] == source_state[i] for i in range(len(variables))]\n",
    "                )\n",
    "            ].copy().reset_index(drop=True)\n",
    "\n",
    "            # for now, sample up to s pairs per variable state\n",
    "            k = min(s, base_df.shape[0] * source_df.shape[0])\n",
    "\n",
    "            pair_indices = random.sample(\n",
    "                list(itertools.product(base_df.index, source_df.index)), k=k\n",
    "            )\n",
    "\n",
    "            for base_i, source_i in pair_indices:\n",
    "                base, source = base_df.iloc[base_i], source_df.iloc[source_i]\n",
    "                base_x, base_mask = self.embed_func(base['Sentence'])\n",
    "                source_x, source_mask = self.embed_func(source['Sentence'])\n",
    "                base_label = base['Prediction']\n",
    "                intervention = self.get_intervention(base, source)\n",
    "                # assuming that gender is a causal variable in our graph\n",
    "                IIT_label = source['Prediction']\n",
    "                data.append((base_x, base_mask, base_label, source_x, source_mask, IIT_label, intervention))\n",
    "\n",
    "        data.sort(key=lambda x: x[-1])\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        base, base_mask, y, source, source_mask, IIT_y, interventions = zip(*data)\n",
    "        self.base = base\n",
    "        self.base_mask = base_mask\n",
    "        self.source = source\n",
    "        self.source_mask = source_mask\n",
    "        self.y = np.array(y)\n",
    "        self.IIT_y = np.array(IIT_y)\n",
    "        self.interventions = np.array(interventions)\n",
    "        return (self.base, self.base_mask), self.y, [(self.source,self.source_mask)], self.IIT_y, self.interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf3ddf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IIT_EEC_dataset(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\"\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        input = [X]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = IIT_EEC_Dataset(\n",
    "        embed_func=encoding,\n",
    "        split=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_dataset()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources,  y_IIT, interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0416dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ii_benchmark import IIBenchmarkMoNli\n",
    "\n",
    "VAR = 0\n",
    "\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "data_size = 4096\n",
    "test_data_size = 256\n",
    "device = 'cuda:0'\n",
    "seed = 42\n",
    "\n",
    "iit_layer = 10 # 8\n",
    "hidden_dim_per_concept = 256\n",
    "\n",
    "intervention_ids_to_coords = {\n",
    "    VAR: [{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "}\n",
    "\n",
    "benchmark = IIBenchmarkMoNli(\n",
    "    variable_names=['GEN'],\n",
    "    data_parameters={\n",
    "        'train_size': data_size, 'test_size': test_data_size\n",
    "    },\n",
    "    model_parameters={\n",
    "        'weights_name': 'bert-base-uncased',\n",
    "        'max_length': 128,\n",
    "        'n_classes': 1,\n",
    "        'hidden_dim': 768,\n",
    "        'target_layers' : [iit_layer],\n",
    "        'target_dims':{\n",
    "            \"start\" : 0,\n",
    "            \"end\" : 786,\n",
    "        },\n",
    "        'debug':False, \n",
    "        'device': device,\n",
    "        'static_search': False,\n",
    "        'nested_disentangle_inplace': False\n",
    "    },\n",
    "    training_parameters={\n",
    "        'warm_start': False, 'max_iter': 10, \n",
    "        'batch_size': 4, #8, \n",
    "        'n_iter_no_change': 10, \n",
    "        'shuffle_train': False, \n",
    "        'eta': 1e-5, # 0.0005, \n",
    "        'device': device        \n",
    "    },\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9866cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "LIM_bert = benchmark.create_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49a9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "LIM_trainer.model.set_analysis_mode(False)\n",
    "# LIM_trainer.model.set_analysis_mode(True) # try to learn alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bdcc50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'saved_models/blackbox-45-42.bin'\n",
    "\n",
    "LIM_trainer.model.load_state_dict(state_dict=torch.load(model_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8881722",
   "metadata": {},
   "source": [
    "Toggle the mode below to switch from DAS (`True`) to CPM (`False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12db258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIM_trainer.model.set_analysis_mode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23f6c152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6917114715216027, pvalue=5.80355572479648e-158)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict(\n",
    "    X_semeval_test\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a856b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasetIIT = get_IIT_EEC_dataset(\n",
    "    data_size=data_size, \n",
    "    split=\"train\",\n",
    "    tokenizer_name='bert-base-uncased'\n",
    ")\n",
    "X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "iit_data = tuple(train_datasetIIT[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abd0b95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\autograd\\__init__.py:173: UserWarning: An output with one or more elements was resized since it had shape [1, 1572, 1572], which does not match the required output shape [1, 1, 1572, 1572]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\utils\\parametrizations.py:83: UserWarning: An output with one or more elements was resized since it had shape [1, 786, 786], which does not match the required output shape [1, 1, 786, 786]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Q = torch.matrix_exp(A)\n",
      "Finished epoch 2 of 10; error is 10.848915926646441"
     ]
    }
   ],
   "source": [
    "_ = LIM_trainer.fit(\n",
    "    X_base_train, \n",
    "    y_base_train, \n",
    "    iit_data=iit_data,\n",
    "    intervention_ids_to_coords=intervention_ids_to_coords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d28afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(LIM_trainer.model.state_dict(), 'iit-eec-no-controls-layer-10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b4c41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'dist-eec-no-controls.pt'\n",
    "\n",
    "LIM_trainer.model.load_state_dict(state_dict=torch.load(model_name))\n",
    "LIM_trainer.model.set_analysis_mode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef12f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sources,  y_IIT, interventions = iit_data\n",
    "\n",
    "# train data eval\n",
    "base_preds_train = LIM_trainer.predict(\n",
    "    X_base_train\n",
    ")\n",
    "IIT_preds_train = LIM_trainer.iit_predict(\n",
    "    X_base_train, X_sources, \n",
    "    interventions, \n",
    "    intervention_ids_to_coords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "951511a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasetIIT = get_IIT_EEC_dataset(\n",
    "    data_size=test_data_size, \n",
    "    split=\"test\",\n",
    "    tokenizer_name='bert-base-uncased'\n",
    ")\n",
    "X_base_test, y_base_test, X_sources_test, y_IIT_test, interventions_test = test_datasetIIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45ecb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data eval\n",
    "base_preds_test = LIM_trainer.predict(\n",
    "    X_base_test\n",
    ")\n",
    "IIT_preds_test = LIM_trainer.iit_predict(\n",
    "    X_base_test, X_sources_test, \n",
    "    interventions_test, \n",
    "    intervention_ids_to_coords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50e43998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on held-out semeval test set (original task)\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "semeval_preds_test = LIM_trainer.predict(\n",
    "    X_semeval_test\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f957b20",
   "metadata": {},
   "source": [
    "### Base Task Metrics\n",
    "\n",
    "How well does the model do at replicating the blackbox model's behavior on the EEC dataset? (**only applies to CPM**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f257d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "6.531900475864827e-13\n",
      "PearsonRResult(statistic=0.999999999990634, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = mean_squared_error(\n",
    "    y_pred=base_preds_train.squeeze().detach().cpu().numpy(), \n",
    "    y_true=y_base_train.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "corr = pearsonr(\n",
    "    base_preds_train.squeeze().detach().cpu().numpy(), \n",
    "    y_base_train.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Train')\n",
    "print(mse)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b84670b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "6.785815118268915e-13\n",
      "PearsonRResult(statistic=0.9999999999904068, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = mean_squared_error(\n",
    "    y_pred=base_preds_test.squeeze().detach().cpu().numpy(), \n",
    "    y_true=y_base_test.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "corr = pearsonr(\n",
    "    base_preds_test.squeeze().detach().cpu().numpy(), \n",
    "    y_base_test.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Test')\n",
    "print(mse)\n",
    "print(corr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e193ac6",
   "metadata": {},
   "source": [
    "### IIT Objective Metrics\n",
    "\n",
    "**Key metric**: does our alignment achieve a strong interchange intervention accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5743fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "0.00240512433708043\n",
      "PearsonRResult(statistic=0.8766649584335187, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = mean_squared_error(\n",
    "    y_pred=IIT_preds_train.squeeze().detach().cpu().numpy(), \n",
    "    y_true=y_IIT.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "corr = pearsonr(\n",
    "    IIT_preds_train.squeeze().detach().cpu().numpy(), \n",
    "    y_IIT.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Train')\n",
    "print(mse)\n",
    "print(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11151975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "0.0024281532063846955\n",
      "PearsonRResult(statistic=0.8777280713553202, pvalue=3.9544240538041496e-83)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = mean_squared_error(\n",
    "    y_pred=IIT_preds_test.squeeze().detach().cpu().numpy(), \n",
    "    y_true=y_IIT_test.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "corr = pearsonr(\n",
    "    IIT_preds_test.squeeze().detach().cpu().numpy(), \n",
    "    y_IIT_test.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Test')\n",
    "print(mse)\n",
    "print(corr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd34662a",
   "metadata": {},
   "source": [
    "### SemEval Test Set\n",
    "\n",
    "How well does the model do at replicating the blackbox model's behavior on the SemEval dataset? (**only applies to CPM**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f25637f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "0.029105594\n",
      "PearsonRResult(statistic=0.666418807902019, pvalue=1.1057104998436334e-142)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = mean_squared_error(\n",
    "    y_pred=semeval_preds_test.squeeze().detach().cpu().numpy(), \n",
    "    y_true=y_semeval_test.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "corr = pearsonr(\n",
    "    semeval_preds_test.squeeze().detach().cpu().numpy(), \n",
    "    y_semeval_test.squeeze().detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "print('Test')\n",
    "print(mse)\n",
    "print(corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ef3f26b",
   "metadata": {},
   "source": [
    "## Debiasing\n",
    "\n",
    "In this section, we take our blackbox model along with our learned alignment, and try to create a debiased version of our model. We do this by intervening on the model's internal representation of gender (as indicated by our alignment) during inference time. Some intervention options include the following:\n",
    "- **Male-Only**: intervene with a static \"male\" vector, computed by averaging over the activations of gender when running our model on EEC sentences with a male noun.\n",
    "- **Female-Only**: similarly, intervene with a static \"female\" vector.\n",
    "- **Null-Out**: intervene with a static zero vector.\n",
    "- **Average**: intervene with a static vector that averages the male and female activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf7e7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IIT_EEC_dataset_by_variable_state(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\"\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        input = [X]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = IIT_EEC_Dataset(\n",
    "        embed_func=encoding,\n",
    "        split=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    data = dataset.create_dataset_by_variable_states()\n",
    "\n",
    "    output = {}\n",
    "    for key in data:\n",
    "        base, base_mask, y_base = zip(*data[key])\n",
    "        X_base = (base, base_mask)\n",
    "        y_base = torch.tensor(y_base)\n",
    "        output[key] = (X_base, y_base)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3947927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_by_variable_state(data_size, gets, tokenizer_name='bert-base-uncased', split='train', batch_size=8):\n",
    "    dataset = get_IIT_EEC_dataset_by_variable_state(data_size, tokenizer_name, split)\n",
    "    output = {}\n",
    "    for key in dataset:\n",
    "        X_base, y_base = dataset[key]\n",
    "        inputs, masks = X_base\n",
    "        inputs, masks = torch.stack(inputs).squeeze(), torch.stack(masks).squeeze()\n",
    "        # print(inputs.shape)\n",
    "        activations = []\n",
    "        with torch.no_grad():\n",
    "            for b in range(0, inputs.size(0), batch_size):\n",
    "                X_batch = (\n",
    "                    inputs[b:b + batch_size].to(LIM_trainer.device), \n",
    "                    masks[b:b + batch_size].to(LIM_trainer.device)\n",
    "                )\n",
    "                # for now (for whatever reason) cut off batch sizes that are too small\n",
    "                if X_batch[0].size(0) < batch_size:\n",
    "                    break\n",
    "                activations_batch, _ = LIM_trainer.model.retrieve_activations(X_batch, gets[0], None)\n",
    "                activations.append(activations_batch)\n",
    "        output[key] = torch.cat(activations, dim=0).mean(dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0978d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervention_logits_by_variable_state(v, data, activations, gets, batch_size=8):\n",
    "    output = {}\n",
    "    for key in data:\n",
    "        output[key] = []\n",
    "        X_base, y_base = data[key]\n",
    "        inputs, masks = X_base\n",
    "        inputs, masks = torch.stack(inputs).squeeze(), torch.stack(masks).squeeze()\n",
    "\n",
    "        sets = copy.deepcopy(gets)\n",
    "        # keep control variables, set sensitive variable\n",
    "        new_key = tuple(list(key)[:-1] + [v])\n",
    "        sets[0]['intervention'] = activations[new_key].repeat((batch_size, 1))\n",
    "\n",
    "        logits = []\n",
    "        with torch.no_grad():\n",
    "            for b in range(0, inputs.size(0), batch_size):\n",
    "                X_batch = (\n",
    "                    inputs[b:b + batch_size].to(LIM_trainer.device), \n",
    "                    masks[b:b + batch_size].to(LIM_trainer.device)\n",
    "                )\n",
    "                # for now cut off batch sizes that are too small\n",
    "                # (can also re-evaluate sets for each batch)\n",
    "                if X_batch[0].size(0) < batch_size:\n",
    "                    break\n",
    "                \n",
    "                _, logits_batch = LIM_trainer.model.retrieve_activations(X_batch, gets[0], sets)\n",
    "                logits.append(logits_batch)\n",
    "        output[key] = {'intervention': torch.cat(logits, dim=0), 'base': y_base.unsqueeze(0)}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef051e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_with_intervention(LIM_trainer, X, gets, intervention, batch_size=8):\n",
    "#     inputs, masks = X\n",
    "#     sets = copy.deepcopy(gets)\n",
    "#     sets[0]['intervention'] = intervention.repeat((batch_size, 1))\n",
    "#     logits = []\n",
    "#     with torch.no_grad():\n",
    "#         for b in range(0, len(inputs), batch_size):\n",
    "#             X_batch = (\n",
    "#                 inputs[b:b + batch_size].to(LIM_trainer.device), \n",
    "#                 masks[b:b + batch_size].to(LIM_trainer.device)\n",
    "#             )\n",
    "\n",
    "#             # adjust to last batch, which might be smaller than batch size\n",
    "#             if len(X_batch[0]) < batch_size:\n",
    "#                 sets[0]['intervention'] = intervention.repeat((len(X_batch[0]), 1))\n",
    "            \n",
    "#             _, logits_batch = LIM_trainer.model.retrieve_activations(X_batch, gets[0], sets)\n",
    "#             logits.append(logits_batch)\n",
    "#     return torch.cat(logits, dim=0)\n",
    "\n",
    "# def evaluate_on_EEC(LIM_trainer, dataset, activations, gets, key):\n",
    "#     pronoun_dataset_m, proper_noun_dataset_m, pronoun_dataset_f, proper_noun_dataset_f = dataset\n",
    "\n",
    "#     predictions_m = predict_with_intervention(LIM_trainer, pronoun_dataset_m, gets, activations[key]) # LIM_trainer.predict(pronoun_dataset_m)\n",
    "#     predictions_f = predict_with_intervention(LIM_trainer, pronoun_dataset_f, gets, activations[key])\n",
    "\n",
    "#     for x_m in zip(*proper_noun_dataset_m):\n",
    "#         predictions_m = torch.cat((\n",
    "#             predictions_m,\n",
    "#             predict_with_intervention(LIM_trainer, x_m, gets, activations[key]).mean().unsqueeze(0)\n",
    "#             # LIM_trainer.predict(x_m).mean().unsqueeze(0)\n",
    "#         ))\n",
    "    \n",
    "#     for x_f in zip(*proper_noun_dataset_f):\n",
    "#         predictions_f = torch.cat((\n",
    "#             predictions_f,\n",
    "#             predict_with_intervention(LIM_trainer, x_f, gets, activations[key]).mean().unsqueeze(0)\n",
    "#         ))\n",
    "    \n",
    "#     return predictions_m, predictions_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd16098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_EEC_with_intervention(LIM_trainer, dataset, gets, intervention):\n",
    "    pronoun_dataset_m, proper_noun_dataset_m, pronoun_dataset_f, proper_noun_dataset_f = dataset\n",
    "\n",
    "    predictions_m = LIM_trainer.predict_with_intervention(pronoun_dataset_m, gets, intervention)\n",
    "    predictions_f = LIM_trainer.predict_with_intervention(pronoun_dataset_f, gets, intervention)\n",
    "\n",
    "    for x_m in zip(*proper_noun_dataset_m):\n",
    "        predictions_m = torch.cat((\n",
    "            predictions_m,\n",
    "            LIM_trainer.predict_with_intervention(x_m, gets, intervention).mean().unsqueeze(0)\n",
    "        ))\n",
    "    \n",
    "    for x_f in zip(*proper_noun_dataset_f):\n",
    "        predictions_f = torch.cat((\n",
    "            predictions_f,\n",
    "            LIM_trainer.predict_with_intervention(x_f, gets, intervention).mean().unsqueeze(0)\n",
    "        ))\n",
    "    \n",
    "    return predictions_m, predictions_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e6dff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR = 0\n",
    "\n",
    "intervention_ids_to_coords = {\n",
    "    VAR: [{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "}\n",
    "\n",
    "data_size = 512\n",
    "gets = intervention_ids_to_coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29aa1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = LIM_trainer.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41dd86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "activations = get_activations_by_variable_state(data_size, gets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f299a391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9911, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(activations[('female',)], activations[('male',)], dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a6946e9",
   "metadata": {},
   "source": [
    "### Male-Only Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a32d88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eec_dataset = get_EEC_dataset('bert-base-uncased')\n",
    "\n",
    "predictions_m, predictions_f = evaluate_on_EEC_with_intervention(LIM_trainer, eec_dataset, gets, activations[('male',)])\n",
    "predictions_m = predictions_m.detach().cpu().numpy()\n",
    "predictions_f = predictions_f.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f95cd758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57980245 0.5785761\n",
      "0.0012263656\n"
     ]
    }
   ],
   "source": [
    "print(predictions_m.mean(), predictions_f.mean())\n",
    "print(predictions_m.mean() - predictions_f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33f23c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5757575757575758\n"
     ]
    }
   ],
   "source": [
    "print((predictions_m > predictions_f).sum() / len(predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c909d1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.3803034463468271, pvalue=0.7038145237641971)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(predictions_m, predictions_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cb89e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6179179769946341, pvalue=3.108180705586764e-117)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict_with_intervention(\n",
    "    X_semeval_test,\n",
    "    gets,\n",
    "    activations[('male',)]\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21f1b890",
   "metadata": {},
   "source": [
    "### Female-Only Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7805f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eec_dataset = get_EEC_dataset('bert-base-uncased')\n",
    "\n",
    "predictions_m, predictions_f = evaluate_on_EEC_with_intervention(LIM_trainer, eec_dataset, gets, activations[('female',)])\n",
    "predictions_m = predictions_m.detach().cpu().numpy()\n",
    "predictions_f = predictions_f.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38cfc2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55353576 0.5469879\n",
      "0.0065478683\n"
     ]
    }
   ],
   "source": [
    "print(predictions_m.mean(), predictions_f.mean())\n",
    "print(predictions_m.mean() - predictions_f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd68de55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print((predictions_m > predictions_f).sum() / len(predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f683c7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=2.2233941516572453, pvalue=0.026449597208438955)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(predictions_m, predictions_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31dba078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6082692999978507, pvalue=1.0860334333683024e-112)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict_with_intervention(\n",
    "    X_semeval_test,\n",
    "    gets,\n",
    "    activations[('female',)]\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a3fcf1e",
   "metadata": {},
   "source": [
    "### Null-Out Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab9c422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eec_dataset = get_EEC_dataset('bert-base-uncased')\n",
    "\n",
    "zero_intervention = torch.zeros_like(activations[('male',)])\n",
    "\n",
    "predictions_m, predictions_f = evaluate_on_EEC_with_intervention(LIM_trainer, eec_dataset, gets, zero_intervention)\n",
    "predictions_m = predictions_m.detach().cpu().numpy()\n",
    "predictions_f = predictions_f.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e53f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5488573 0.5586735\n",
      "-0.009816229\n"
     ]
    }
   ],
   "source": [
    "print(predictions_m.mean(), predictions_f.mean())\n",
    "print(predictions_m.mean() - predictions_f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33777f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37995337995337997\n"
     ]
    }
   ],
   "source": [
    "print((predictions_m > predictions_f).sum() / len(predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa7fc7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.6679353360988525, pvalue=0.09569429862791945)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(predictions_m, predictions_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ae25527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6550148348399034, pvalue=2.8304215645763145e-136)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict_with_intervention(\n",
    "    X_semeval_test,\n",
    "    gets,\n",
    "    zero_intervention\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27f2597a",
   "metadata": {},
   "source": [
    "### Average of Representations Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16ac18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eec_dataset = get_EEC_dataset('bert-base-uncased')\n",
    "\n",
    "avg_intervention = torch.mean(torch.stack((list(activations.values()))), dim=0)\n",
    "\n",
    "predictions_m, predictions_f = evaluate_on_EEC_with_intervention(LIM_trainer, eec_dataset, gets, avg_intervention)\n",
    "predictions_m = predictions_m.detach().cpu().numpy()\n",
    "predictions_f = predictions_f.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "039f813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5664269 0.5625457\n",
      "0.0038811564\n"
     ]
    }
   ],
   "source": [
    "print(predictions_m.mean(), predictions_f.mean())\n",
    "print(predictions_m.mean() - predictions_f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd5ef5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6177156177156177\n"
     ]
    }
   ],
   "source": [
    "print((predictions_m > predictions_f).sum() / len(predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d413bc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.2597623711441954, pvalue=0.20809845268978142)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(predictions_m, predictions_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7253722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.6135130867062852, pvalue=3.8564857094479446e-115)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict_with_intervention(\n",
    "    X_semeval_test,\n",
    "    gets,\n",
    "    avg_intervention\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3165da8b",
   "metadata": {},
   "source": [
    "### No Intervention (Should Replicate Original Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "892b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_m, predictions_f = evaluate_on_EEC(LIM_trainer, eec_dataset)\n",
    "predictions_m = predictions_m.detach().cpu().numpy()\n",
    "predictions_f = predictions_f.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d7a6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5495071 0.50484985\n",
      "0.04465723\n"
     ]
    }
   ],
   "source": [
    "print(predictions_m.mean(), predictions_f.mean())\n",
    "print(predictions_m.mean() - predictions_f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0dc38192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "print((predictions_m > predictions_f).sum() / len(predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fddc2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=5.596013001291422, pvalue=2.9528760836644173e-08)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(predictions_m, predictions_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26bf4079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.666418807902019, pvalue=1.1057104998436334e-142)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "X_semeval_test, y_semeval_test = get_semeval_dataset(test_df, 'bert-base-uncased', test_df.shape[0] - 1)\n",
    "predictions = LIM_trainer.predict(\n",
    "    X_semeval_test\n",
    ")\n",
    "\n",
    "pearsonr(predictions.cpu().numpy(), y_semeval_test.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interchange",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d283f86b5d2fbc31f35f78f73be4d0bb5be67dfbb54fdd34f287f29d60f844e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
