{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II-Benchmark\n",
    "\n",
    "A benchmark for causal abstraction-like analyses, which try to find an alignment between a model's computation and a corresponding causal graph.\n",
    "This benchmark supplies a variety of models, trained using IIT to localize causal concepts in the hierarchical equality task.\n",
    "\n",
    "I broke down the repository to two parts:\n",
    "* __Generating models__: behind the curtains, trains a set of models on a variety of alignments on a single training set of the equality task.\n",
    "* __Evaluating a model__: for those who wish to benchmark themselves, provides a \"blackbox\" model trained by IIT and evaluates alignment on the black-box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# admittedly takes a bit, because it loads the training and testing data sets\n",
    "from ii_benchmark import IIBenchmarkEquality, IIBenchmarkMoNli\n",
    "\n",
    "benchmark = IIBenchmarkEquality()\n",
    "\n",
    "V1 = 0\n",
    "V2 = 1\n",
    "BOTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Models\n",
    "\n",
    "Generates models for each alignment between causal variables and neural activations, sampled from exhaustive generation of all possible alignments.\n",
    "For each sampled alignment, we train three models: one that aligns only V1, one that aligns only V2, and one that aligns BOTH.\n",
    "\n",
    "Models are saved in the `./models/` repository, and are named by the following convention: the variable `v` (one of V1, V2, or BOTH), the intervention location for V1 (layer, start index, end index), and the intervention location for V2 (layer, start index, end index).\n",
    "\n",
    "__NOTE__: currently, I am mapping a causal variable like V1 to a _contiguous block of neural activations_. I wonder if we should try to create distributed mappings? For instance, V1 can map to indices 1:3 and 5:7 in layer 1's activation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample `n` alignments for training models for the benchmark\n",
    "n = 2\n",
    "alignments = benchmark.sample_alignments(n_samples=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model using IIT for each alignment (and for each variable in V1, V2, and BOTH)\n",
    "# NOTE: currently commented out, because this is a time-consuming step that should only be taken once\n",
    "benchmark.train_models(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models\n",
    "\n",
    "Loads a model from our list of models, and evaluates possible alignments on the model using interchange interventions.\n",
    "\n",
    "Small note: right now, our evaluation kind of \"gives away\" the alignment by the name of the weights files. This should be a small fix, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/v=2v1=1-3-4v2=1-4-14.pt'\n",
    "blackbox_LIM = benchmark.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alignment = {\n",
    "    V1: [{'layer': 1, 'start': 3, 'end': 4}],\n",
    "    V2: [{'layer': 1, 'start': 4, 'end': 14}],\n",
    "    BOTH: [{'layer': 1, 'start': 3, 'end': 4}, {'layer': 1, 'start': 4, 'end': 14}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "II-Evaluation on V1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       500\n",
      "           1       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "II-Evaluation on V2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       500\n",
      "           1       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "II-Evaluation on BOTH\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.52      0.51       498\n",
      "           1       0.50      0.48      0.49       502\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.50      0.50      0.50      1000\n",
      "weighted avg       0.50      0.50      0.50      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation = benchmark.evaluate(blackbox_LIM, true_alignment)\n",
    "benchmark.display_evaluations(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_alignment = {\n",
    "    V1: [{'layer': 2, 'start': 3, 'end': 4}],\n",
    "    V2: [{'layer': 2, 'start': 4, 'end': 14}],\n",
    "    BOTH: [{'layer': 1, 'start': 4, 'end': 14}, {'layer': 1, 'start': 3, 'end': 4}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "II-Evaluation on V1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       500\n",
      "           1       0.50      0.50      0.50       500\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.50      0.50      0.50      1000\n",
      "weighted avg       0.50      0.50      0.50      1000\n",
      "\n",
      "II-Evaluation on V2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.55      0.54       500\n",
      "           1       0.53      0.50      0.52       500\n",
      "\n",
      "    accuracy                           0.53      1000\n",
      "   macro avg       0.53      0.53      0.53      1000\n",
      "weighted avg       0.53      0.53      0.53      1000\n",
      "\n",
      "II-Evaluation on BOTH\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.52      0.51       498\n",
      "           1       0.50      0.48      0.49       502\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.50      0.50      0.50      1000\n",
      "weighted avg       0.50      0.50      0.50      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation = benchmark.evaluate(blackbox_LIM, bad_alignment)\n",
    "benchmark.display_evaluations(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d283f86b5d2fbc31f35f78f73be4d0bb5be67dfbb54fdd34f287f29d60f844e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
