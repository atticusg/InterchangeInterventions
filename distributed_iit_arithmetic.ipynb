{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import torch, gc\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import utils\n",
    "from trainer import LIMTrainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from LIM_deep_neural_classifier import LIMDeepNeuralClassifier\n",
    "import dataset_equality\n",
    "import os.path\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "utils.fix_random_seeds()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def ab_c(a, b, c):\n",
    "    return (a+b)*c\n",
    "\n",
    "def get_IIT_arithmetic_dataset_factuals(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    random.seed(seed)\n",
    "    base = [[random.randint(\n",
    "        variable_range[0], variable_range[1]\n",
    "    ) for _ in range(3)] for _ in range(data_size)]\n",
    "    base_y = []\n",
    "    for b in base:\n",
    "        b_y = combiner_func(*b)\n",
    "        base_y += [b_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), torch.tensor(base_y, dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_factual_pairs(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    random.seed(seed)\n",
    "    base = [[random.randint(\n",
    "        variable_range[0], variable_range[1]\n",
    "    ) for _ in range(3)] for _ in range(data_size)]\n",
    "    base_y = []\n",
    "    for b in base:\n",
    "        b_y = combiner_func(*b)\n",
    "        base_y += [b_y]\n",
    "        \n",
    "    source = [[random.randint(\n",
    "        variable_range[0], variable_range[1]\n",
    "    ) for _ in range(3)] for _ in range(data_size)]\n",
    "    source_y = []\n",
    "    for s in source:\n",
    "        s_y = combiner_func(*s)\n",
    "        source_y += [s_y]\n",
    "        \n",
    "    return base, \\\n",
    "        base_y, \\\n",
    "        source, \\\n",
    "        source_y\n",
    "\n",
    "def get_IIT_arithmetic_dataset_sum_first_V1(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base, base_y, source, source_y = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    source_sum = []\n",
    "    for s in source:\n",
    "        s_sum = s[0]+s[1]\n",
    "        source_sum += [s_sum]\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(base)):\n",
    "        c_y = source_sum[i]*base[i][-1]\n",
    "        counterfactual_y += [c_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), \\\n",
    "        torch.tensor(base_y, dtype=torch.long), \\\n",
    "        [torch.tensor(source, dtype=torch.long), torch.tensor(source, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([0 for _ in range(len(base))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_sum_first_V2(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base, base_y, source, source_y = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(base)):\n",
    "        c_y = (base[i][0]+base[i][1])*source[i][-1]\n",
    "        counterfactual_y += [c_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), \\\n",
    "        torch.tensor(base_y, dtype=torch.long), \\\n",
    "        [torch.tensor(source, dtype=torch.long), torch.tensor(source, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([1 for _ in range(len(base))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_sum_first_V1_V2(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base_1, base_y_1, source_1, source_y_1 = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    base_2, base_y_2, source_2, source_y_2 = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed+1,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    source_sum_1 = []\n",
    "    for s in source_1:\n",
    "        s_sum = s[0]+s[1]\n",
    "        source_sum_1 += [s_sum]\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(source_2)):\n",
    "        c_y = source_sum_1[i]*source_2[i][-1]\n",
    "        counterfactual_y += [c_y]\n",
    "    \n",
    "    return torch.tensor(base_1, dtype=torch.long), \\\n",
    "        torch.tensor(base_y_1, dtype=torch.long), \\\n",
    "        [torch.tensor(source_1, dtype=torch.long), torch.tensor(source_2, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([2 for _ in range(len(base_1))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_sum_first(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    V1_dataset = get_IIT_arithmetic_dataset_sum_first_V1(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    V2_dataset = get_IIT_arithmetic_dataset_sum_first_V2(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    both_dataset = get_IIT_arithmetic_dataset_sum_first_V1_V2(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    combined_dataset = [torch.cat((V1_dataset[0],\n",
    "                                    V2_dataset[0],\n",
    "                                    both_dataset[0])),\n",
    "                       torch.cat((V1_dataset[1],\n",
    "                                    V2_dataset[1],\n",
    "                                    both_dataset[1])),\n",
    "                       [torch.cat((V1_dataset[2][0],\n",
    "                                    V2_dataset[2][0],\n",
    "                                    both_dataset[2][0])),\n",
    "                       torch.cat((V1_dataset[2][0],\n",
    "                                    V2_dataset[2][0],\n",
    "                                    both_dataset[2][1]))],\n",
    "                       torch.cat((V1_dataset[3],\n",
    "                                    V2_dataset[3],\n",
    "                                    both_dataset[3])),\n",
    "                       torch.cat((V1_dataset[4],\n",
    "                                    V2_dataset[4],\n",
    "                                    both_dataset[4]))]\n",
    "    return combined_dataset\n",
    "\n",
    "def get_IIT_arithmetic_dataset_sum_first_control_V1(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base, base_y, source, source_y = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    source_sum = []\n",
    "    for s in source:\n",
    "        s_sum = s[0]+s[1]\n",
    "        source_sum += [s_sum]\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(base)):\n",
    "        c_y = source_sum[i]*base[i][-1]\n",
    "        counterfactual_y += [c_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), \\\n",
    "        torch.tensor(base_y, dtype=torch.long), \\\n",
    "        [torch.tensor(source, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([3 for _ in range(len(base))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_control_V1(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base, base_y, source, source_y = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(base)):\n",
    "        c_y = combiner_func(source[i][0], base[i][1], base[i][-1])\n",
    "        counterfactual_y += [c_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), \\\n",
    "        torch.tensor(base_y, dtype=torch.long), \\\n",
    "        [torch.tensor(source, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([3 for _ in range(len(base))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_prod_first_V1(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base, base_y, source, source_y = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    source_prod = []\n",
    "    for s in source:\n",
    "        s_prod = s[0]*s[2]\n",
    "        source_prod += [s_prod]\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(base)):\n",
    "        c_y = source_prod[i] + (base[i][1]*base[i][2])\n",
    "        counterfactual_y += [c_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), \\\n",
    "        torch.tensor(base_y, dtype=torch.long), \\\n",
    "        [torch.tensor(source, dtype=torch.long), torch.tensor(source, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([0 for _ in range(len(base))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_prod_first_V2(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base, base_y, source, source_y = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    source_prod = []\n",
    "    for s in source:\n",
    "        s_prod = s[1]*s[2]\n",
    "        source_prod += [s_prod]\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(base)):\n",
    "        c_y = source_prod[i] + (base[i][0]*base[i][2])\n",
    "        counterfactual_y += [c_y]\n",
    "        \n",
    "    return torch.tensor(base, dtype=torch.long), \\\n",
    "        torch.tensor(base_y, dtype=torch.long), \\\n",
    "        [torch.tensor(source, dtype=torch.long), torch.tensor(source, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([1 for _ in range(len(base))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_prod_first_V1_V2(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    base_1, base_y_1, source_1, source_y_1 = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    base_2, base_y_2, source_2, source_y_2 = get_IIT_arithmetic_dataset_factual_pairs(\n",
    "        variable_range, seed+1,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    \n",
    "    source_prod_1 = []\n",
    "    for s in source_1:\n",
    "        s_prod = s[0]*s[2]\n",
    "        source_prod_1 += [s_prod]\n",
    "    \n",
    "    counterfactual_y = []\n",
    "    for i in range(len(source_2)):\n",
    "        c_y = source_prod_1[i] + (source_2[i][1]*source_2[i][2])\n",
    "        counterfactual_y += [c_y]\n",
    "    \n",
    "    return torch.tensor(base_1, dtype=torch.long), \\\n",
    "        torch.tensor(base_y_1, dtype=torch.long), \\\n",
    "        [torch.tensor(source_1, dtype=torch.long), torch.tensor(source_2, dtype=torch.long)], \\\n",
    "        torch.tensor(counterfactual_y, dtype=torch.long), \\\n",
    "        torch.tensor([2 for _ in range(len(base_1))], dtype=torch.long)\n",
    "\n",
    "def get_IIT_arithmetic_dataset_prod_first(\n",
    "    variable_range, seed,\n",
    "    data_size,\n",
    "    combiner_func\n",
    "):\n",
    "    V1_dataset = get_IIT_arithmetic_dataset_prod_first_V1(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    V2_dataset = get_IIT_arithmetic_dataset_prod_first_V2(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    both_dataset = get_IIT_arithmetic_dataset_prod_first_V1_V2(\n",
    "        variable_range, seed,\n",
    "        data_size,\n",
    "        combiner_func\n",
    "    )\n",
    "    combined_dataset = [torch.cat((V1_dataset[0],\n",
    "                                    V2_dataset[0],\n",
    "                                    both_dataset[0])),\n",
    "                       torch.cat((V1_dataset[1],\n",
    "                                    V2_dataset[1],\n",
    "                                    both_dataset[1])),\n",
    "                       [torch.cat((V1_dataset[2][0],\n",
    "                                    V2_dataset[2][0],\n",
    "                                    both_dataset[2][0])),\n",
    "                       torch.cat((V1_dataset[2][0],\n",
    "                                    V2_dataset[2][0],\n",
    "                                    both_dataset[2][1]))],\n",
    "                       torch.cat((V1_dataset[3],\n",
    "                                    V2_dataset[3],\n",
    "                                    both_dataset[3])),\n",
    "                       torch.cat((V1_dataset[4],\n",
    "                                    V2_dataset[4],\n",
    "                                    both_dataset[4]))]\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Factual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed variables\n",
    "device = \"cpu\"\n",
    "data_size = 6400\n",
    "num_layers = 3\n",
    "max_iter = 500\n",
    "variable_range = [1, 6]\n",
    "\n",
    "embedding_dim = variable_range[1]-variable_range[0]+1\n",
    "input_dim = embedding_dim * 3\n",
    "min_y = (variable_range[0]+variable_range[0])*variable_range[0]\n",
    "max_y = (variable_range[1]+variable_range[1])*variable_range[1]\n",
    "classes = sorted(set([i for i in range(min_y, max_y+1)]))\n",
    "n_classes = (variable_range[1]+variable_range[1])*variable_range[1]\n",
    "class2index = dict(zip(classes, range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in {42, 77, 88}:\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        utils.fix_random_seeds(seed=seed)\n",
    "        print(f\"training factual model for seed={seed}\")\n",
    "        X_base_train, y_base_train = get_IIT_arithmetic_dataset_factuals(\n",
    "            variable_range, seed, data_size, ab_c\n",
    "        )\n",
    "        LIM = LIMDeepNeuralClassifier(\n",
    "            hidden_dim=hidden_dim, \n",
    "            hidden_activation=torch.nn.ReLU(), \n",
    "            num_layers=num_layers,\n",
    "            input_dim=input_dim,\n",
    "            n_classes=n_classes,\n",
    "            device=device,\n",
    "            vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "            embed_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        LIM_trainer = LIMTrainer(\n",
    "            LIM,\n",
    "            warm_start=True,\n",
    "            max_iter=max_iter,\n",
    "            batch_size=6400,\n",
    "            n_iter_no_change=10000,\n",
    "            shuffle_train=False,\n",
    "            eta=0.01,\n",
    "            input_as_ids=True,\n",
    "            device=device,\n",
    "            class2index=class2index,\n",
    "            save_checkpoint_per_epoch=True,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        _ = LIM_trainer.fit(\n",
    "            X_base_train, \n",
    "            y_base_train, \n",
    "            iit_data=None,\n",
    "            intervention_ids_to_coords=None\n",
    "        )\n",
    "        \n",
    "        PATH = f\"./saved_models_arithmetic/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "        torch.save(LIM_trainer.model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIT Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Oracle Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fun = get_IIT_arithmetic_dataset_sum_first\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 135\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 6}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                small_scale_factor = hidden_dim/18\n",
    "                id_to_coords = {\n",
    "                    0: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}],\n",
    "                    1: [{\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor+small_scale_factor)*hidden_dim_per_concept)}],\n",
    "                    2: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}, \n",
    "                        {\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor+small_scale_factor)*hidden_dim_per_concept)}],\n",
    "                }\n",
    "                print(\"id_to_coords: \", id_to_coords)\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    \n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-oraclemodel-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    if os.path.isfile(PATH):\n",
    "                        print(f\"Found trained model thus skip: {PATH}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Training {model_c}/{total_model_c} model aligned with oracle hlm with params:\")\n",
    "                    print(f\"seed={seed}\")\n",
    "                    print(f\"hidden_dim_per_concept={hidden_dim_per_concept}\")\n",
    "                    print(f\"iit_layer={iit_layer}\")\n",
    "                    print(f\"epoch={i}\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "        \n",
    "                    ORACLE_PATH = f\"./saved_models_arithmetic/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(ORACLE_PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "\n",
    "                    _ = LIM_trainer.fit(\n",
    "                        X_base_train, \n",
    "                        y_base_train, \n",
    "                        iit_data=iit_data,\n",
    "                        intervention_ids_to_coords=id_to_coords)\n",
    "\n",
    "                    torch.save(LIM_trainer.model.state_dict(), PATH)\n",
    "\n",
    "                    model_c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Oracle Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_results = []\n",
    "dataset_fun = get_IIT_arithmetic_dataset_sum_first\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 135\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        utils.get_eval_from_train(\n",
    "            train_datasetIIT, 6400\n",
    "        )\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = \\\n",
    "        dataset_fun(\n",
    "            variable_range, seed+1, 6400, ab_c\n",
    "        )\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 6}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                small_scale_factor = hidden_dim/18\n",
    "                id_to_coords = {\n",
    "                    0: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}],\n",
    "                    1: [{\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor+small_scale_factor)*hidden_dim_per_concept)}],\n",
    "                    2: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}, \n",
    "                        {\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor+small_scale_factor)*hidden_dim_per_concept)}],\n",
    "                }\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    print(f\"Evaluating {model_c}/270 model aligned with oracle hlm.\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "\n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-oraclemodel-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "                    \n",
    "                    # train data eval\n",
    "                    base_preds_train = LIM_trainer.predict(\n",
    "                        base_test_train, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                        base_test_train, sources_test_train, \n",
    "                        intervention_ids_test_train, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1_train = classification_report(y_base_test_train, base_preds_train, output_dict=True)\n",
    "                    r2_train = classification_report(y_IIT_test_train, IIT_preds_train, output_dict=True)\n",
    "\n",
    "                    # test data eval\n",
    "                    base_preds = LIM_trainer.predict(\n",
    "                        base_test, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds = LIM_trainer.iit_predict(\n",
    "                        base_test, sources_test, \n",
    "                        intervention_ids_test, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1 = classification_report(y_base_test, base_preds, output_dict=True)\n",
    "                    r2 = classification_report(y_IIT_test, IIT_preds, output_dict=True)\n",
    "\n",
    "                    oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    model_c += 1\n",
    "oracle_df = pd.DataFrame(\n",
    "    oracle_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (0, 1) control model with d-IIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fun = get_IIT_arithmetic_dataset_sum_first_control_V1\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 270\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 6}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                small_scale_factor = hidden_dim/18\n",
    "                control = 3\n",
    "                id_to_coords = {\n",
    "                    control: [\n",
    "                        {\n",
    "                            \"layer\": iit_layer, \n",
    "                            \"start\": 0, \n",
    "                            \"end\": int(scale_factor*hidden_dim_per_concept)\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                print(\"id_to_coords: \", id_to_coords)\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    \n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-controlmodel-0-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    if os.path.isfile(PATH):\n",
    "                        print(f\"Found trained model thus skip: {PATH}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Training {model_c}/{total_model_c} model aligned with oracle hlm with params:\")\n",
    "                    print(f\"seed={seed}\")\n",
    "                    print(f\"hidden_dim_per_concept={hidden_dim_per_concept}\")\n",
    "                    print(f\"iit_layer={iit_layer}\")\n",
    "                    print(f\"epoch={i}\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "        \n",
    "                    ORACLE_PATH = f\"./saved_models_arithmetic/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(ORACLE_PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "\n",
    "                    _ = LIM_trainer.fit(\n",
    "                        X_base_train, \n",
    "                        y_base_train, \n",
    "                        iit_data=iit_data,\n",
    "                        intervention_ids_to_coords=id_to_coords)\n",
    "\n",
    "                    torch.save(LIM_trainer.model.state_dict(), PATH)\n",
    "\n",
    "                    model_c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval (0, 1) control model with d-IIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_0_results = []\n",
    "dataset_fun = get_IIT_arithmetic_dataset_sum_first_control_V1\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 135\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        utils.get_eval_from_train(\n",
    "            train_datasetIIT, 6400, control=True\n",
    "        )\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = \\\n",
    "        dataset_fun(\n",
    "            variable_range, seed+1, 6400, ab_c\n",
    "        )\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 6}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                small_scale_factor = hidden_dim/18\n",
    "                control = 3\n",
    "                id_to_coords = {\n",
    "                    control: [\n",
    "                        {\n",
    "                            \"layer\": iit_layer, \n",
    "                            \"start\": 0, \n",
    "                            \"end\": int(scale_factor*hidden_dim_per_concept)\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    print(f\"Evaluating {model_c}/270 model aligned with oracle hlm.\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "\n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-controlmodel-0-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "                    \n",
    "                    # train data eval\n",
    "                    base_preds_train = LIM_trainer.predict(\n",
    "                        base_test_train, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                        base_test_train, sources_test_train, \n",
    "                        intervention_ids_test_train, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1_train = classification_report(y_base_test_train, base_preds_train, output_dict=True)\n",
    "                    r2_train = classification_report(y_IIT_test_train, IIT_preds_train, output_dict=True)\n",
    "\n",
    "                    # test data eval\n",
    "                    base_preds = LIM_trainer.predict(\n",
    "                        base_test, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds = LIM_trainer.iit_predict(\n",
    "                        base_test, sources_test, \n",
    "                        intervention_ids_test, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1 = classification_report(y_base_test, base_preds, output_dict=True)\n",
    "                    r2 = classification_report(y_IIT_test, IIT_preds, output_dict=True)\n",
    "\n",
    "                    control_0_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_0_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_0_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_0_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    model_c += 1\n",
    "control_0_df = pd.DataFrame(\n",
    "    control_0_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (0, ) control model with d-IIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fun = get_IIT_arithmetic_dataset_control_V1\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 270\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 6}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                small_scale_factor = hidden_dim/18\n",
    "                control = 3\n",
    "                id_to_coords = {\n",
    "                    control: [\n",
    "                        {\n",
    "                            \"layer\": iit_layer, \n",
    "                            \"start\": 0, \n",
    "                            \"end\": int(scale_factor*hidden_dim_per_concept*0.5)\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                print(\"id_to_coords: \", id_to_coords)\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    \n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-controlmodel-1-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    if os.path.isfile(PATH):\n",
    "                        print(f\"Found trained model thus skip: {PATH}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Training {model_c}/{total_model_c} model aligned with oracle hlm with params:\")\n",
    "                    print(f\"seed={seed}\")\n",
    "                    print(f\"hidden_dim_per_concept={hidden_dim_per_concept}\")\n",
    "                    print(f\"iit_layer={iit_layer}\")\n",
    "                    print(f\"epoch={i}\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "        \n",
    "                    ORACLE_PATH = f\"./saved_models_arithmetic/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(ORACLE_PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "\n",
    "                    _ = LIM_trainer.fit(\n",
    "                        X_base_train, \n",
    "                        y_base_train, \n",
    "                        iit_data=iit_data,\n",
    "                        intervention_ids_to_coords=id_to_coords)\n",
    "\n",
    "                    torch.save(LIM_trainer.model.state_dict(), PATH)\n",
    "\n",
    "                    model_c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval (0, ) control model with d-IIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_1_results = []\n",
    "dataset_fun = get_IIT_arithmetic_dataset_control_V1\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 135\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        utils.get_eval_from_train(\n",
    "            train_datasetIIT, 6400, control=True\n",
    "        )\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = \\\n",
    "        dataset_fun(\n",
    "            variable_range, seed+1, 6400, ab_c\n",
    "        )\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 6}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                small_scale_factor = hidden_dim/18\n",
    "                control = 3\n",
    "                id_to_coords = {\n",
    "                    control: [\n",
    "                        {\n",
    "                            \"layer\": iit_layer, \n",
    "                            \"start\": 0, \n",
    "                            \"end\": int(scale_factor*hidden_dim_per_concept*0.5)\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    print(f\"Evaluating {model_c}/270 model aligned with oracle hlm.\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "\n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-controlmodel-1-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "                    \n",
    "                    # train data eval\n",
    "                    base_preds_train = LIM_trainer.predict(\n",
    "                        base_test_train, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                        base_test_train, sources_test_train, \n",
    "                        intervention_ids_test_train, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1_train = classification_report(y_base_test_train, base_preds_train, output_dict=True)\n",
    "                    r2_train = classification_report(y_IIT_test_train, IIT_preds_train, output_dict=True)\n",
    "\n",
    "                    # test data eval\n",
    "                    base_preds = LIM_trainer.predict(\n",
    "                        base_test, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds = LIM_trainer.iit_predict(\n",
    "                        base_test, sources_test, \n",
    "                        intervention_ids_test, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1 = classification_report(y_base_test, base_preds, output_dict=True)\n",
    "                    r2 = classification_report(y_IIT_test, IIT_preds, output_dict=True)\n",
    "\n",
    "                    control_1_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_1_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_1_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_1_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    model_c += 1\n",
    "control_1_df = pd.DataFrame(\n",
    "    control_1_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train distributed rule oracle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fun = get_IIT_arithmetic_dataset_prod_first\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 135\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 4.5}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                id_to_coords = {\n",
    "                    0: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}],\n",
    "                    1: [{\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor*2)*hidden_dim_per_concept)}],\n",
    "                    2: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}, \n",
    "                        {\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor*2)*hidden_dim_per_concept)}],\n",
    "                }\n",
    "                print(\"id_to_coords: \", id_to_coords)\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    \n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-prod-oraclemodel-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    if os.path.isfile(PATH):\n",
    "                        print(f\"Found trained model thus skip: {PATH}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Training {model_c}/{total_model_c} model aligned with oracle hlm with params:\")\n",
    "                    print(f\"seed={seed}\")\n",
    "                    print(f\"hidden_dim_per_concept={hidden_dim_per_concept}\")\n",
    "                    print(f\"iit_layer={iit_layer}\")\n",
    "                    print(f\"epoch={i}\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "        \n",
    "                    ORACLE_PATH = f\"./saved_models_arithmetic/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(ORACLE_PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "\n",
    "                    _ = LIM_trainer.fit(\n",
    "                        X_base_train, \n",
    "                        y_base_train, \n",
    "                        iit_data=iit_data,\n",
    "                        intervention_ids_to_coords=id_to_coords)\n",
    "\n",
    "                    torch.save(LIM_trainer.model.state_dict(), PATH)\n",
    "\n",
    "                    model_c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval distributed rule oracle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_oracle_results = []\n",
    "dataset_fun = get_IIT_arithmetic_dataset_prod_first\n",
    "iit_data_size = 64000\n",
    "iit_max_iter = 50\n",
    "model_c = 1\n",
    "total_model_c = 135\n",
    "for seed in {42, 77, 88}: # {42, 77, 88}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = dataset_fun(\n",
    "        variable_range, seed, iit_data_size, ab_c\n",
    "    )\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        utils.get_eval_from_train(\n",
    "            train_datasetIIT, 6400\n",
    "        )\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = \\\n",
    "        dataset_fun(\n",
    "            variable_range, seed+1, 6400, ab_c\n",
    "        )\n",
    "    for hidden_dim in {18, 36}: # {18, 36}\n",
    "        for hidden_dim_per_concept in {1, 2, 4.5}: # {1, 2, 6}\n",
    "            for iit_layer in [0, 1, 2]: # [0, 1, 2]\n",
    "                scale_factor = hidden_dim/9\n",
    "                id_to_coords = {\n",
    "                    0: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}],\n",
    "                    1: [{\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor*2)*hidden_dim_per_concept)}],\n",
    "                    2: [{\"layer\": iit_layer, \"start\": 0, \"end\": int(scale_factor*hidden_dim_per_concept)}, \n",
    "                        {\"layer\": iit_layer, \"start\":  int(scale_factor*hidden_dim_per_concept), \"end\": int((scale_factor*2)*hidden_dim_per_concept)}],\n",
    "                }\n",
    "                for i in [100, 200, 300, 400, 500]:\n",
    "                    print(f\"Evaluating {model_c}/270 model aligned with oracle hlm.\")\n",
    "                    LIM = LIMDeepNeuralClassifier(\n",
    "                        hidden_dim=hidden_dim, \n",
    "                        hidden_activation=torch.nn.ReLU(), \n",
    "                        num_layers=num_layers,\n",
    "                        input_dim=input_dim,\n",
    "                        n_classes=n_classes,\n",
    "                        device=device,\n",
    "                        vocab_size=variable_range[1]-variable_range[0]+1,\n",
    "                        embed_dim=embedding_dim,\n",
    "                    )\n",
    "                    LIM_trainer = LIMTrainer(\n",
    "                        LIM,\n",
    "                        warm_start=True,\n",
    "                        max_iter=iit_max_iter,\n",
    "                        batch_size=6400,\n",
    "                        n_iter_no_change=10000,\n",
    "                        shuffle_train=False,\n",
    "                        eta=0.01,\n",
    "                        input_as_ids=True,\n",
    "                        device=device,\n",
    "                        class2index=class2index,\n",
    "                        save_checkpoint_per_epoch=False,\n",
    "                        seed=seed\n",
    "                    )\n",
    "\n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    PATH = f\"./saved_models_arithmetic/iit-prod-oraclemodel-epoch{i}-\"\\\n",
    "                           f\"{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-\"\\\n",
    "                           f\"{seed}.bin\"\n",
    "                    LIM_trainer.model.load_state_dict(torch.load(PATH))\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "                    \n",
    "                    # train data eval\n",
    "                    base_preds_train = LIM_trainer.predict(\n",
    "                        base_test_train, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                        base_test_train, sources_test_train, \n",
    "                        intervention_ids_test_train, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1_train = classification_report(y_base_test_train, base_preds_train, output_dict=True)\n",
    "                    r2_train = classification_report(y_IIT_test_train, IIT_preds_train, output_dict=True)\n",
    "\n",
    "                    # test data eval\n",
    "                    base_preds = LIM_trainer.predict(\n",
    "                        base_test, device=\"cpu\"\n",
    "                    )\n",
    "                    IIT_preds = LIM_trainer.iit_predict(\n",
    "                        base_test, sources_test, \n",
    "                        intervention_ids_test, \n",
    "                        id_to_coords, device=\"cpu\"\n",
    "                    )\n",
    "                    r1 = classification_report(y_base_test, base_preds, output_dict=True)\n",
    "                    r2 = classification_report(y_IIT_test, IIT_preds, output_dict=True)\n",
    "\n",
    "                    prod_oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    prod_oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    prod_oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    prod_oracle_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    model_c += 1\n",
    "prod_oracle_df = pd.DataFrame(\n",
    "    prod_oracle_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=control_1_df[\n",
    "        (control_1_df[\"hidden_dim\"]==36)&\n",
    "        (control_1_df[\"hidden_dim_per_concept\"]==6)&\n",
    "        (control_1_df[\"iit_layer\"]==1)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=control_0_df[\n",
    "        (control_0_df[\"hidden_dim\"]==36)&\n",
    "        (control_0_df[\"hidden_dim_per_concept\"]==6)&\n",
    "        (control_0_df[\"iit_layer\"]==1)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=oracle_df[\n",
    "        (oracle_df[\"hidden_dim\"]==36)&\n",
    "        (oracle_df[\"hidden_dim_per_concept\"]==6)&\n",
    "        (oracle_df[\"iit_layer\"]==1)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=prod_oracle_df[\n",
    "        (prod_oracle_df[\"hidden_dim\"]==36)&\n",
    "        (prod_oracle_df[\"hidden_dim_per_concept\"]==4.5)&\n",
    "        (prod_oracle_df[\"iit_layer\"]==1)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
