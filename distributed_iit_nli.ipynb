{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b777b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_nli\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import random, os\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from LIM_bert import LIMBERTClassifier\n",
    "from ii_benchmark import IIBenchmarkMoNli\n",
    "\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d497368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_from_train_nli(iit_nli_dataset, n=1000, control=False):\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = iit_nli_dataset\n",
    "    \n",
    "    if control:\n",
    "        if len(sources_test) == 1:\n",
    "            \n",
    "            \n",
    "            indices = torch.randperm(len(base_test[0])) # within each bucket, we randomize!\n",
    "            indices = indices[:n]\n",
    "            \n",
    "            return (\n",
    "                (\n",
    "                    [base_test[0][ind] for ind in indices], \n",
    "                    [base_test[1][ind] for ind in indices], \n",
    "                ), \n",
    "                y_base_test[indices], \n",
    "                [\n",
    "                    (\n",
    "                        [sources_test[0][0][ind] for ind in indices],\n",
    "                        [sources_test[0][1][ind] for ind in indices]\n",
    "                    )\n",
    "                ], \n",
    "                y_IIT_test[indices], \n",
    "                intervention_ids_test[indices], \n",
    "            )\n",
    "        else:\n",
    "            \n",
    "            assert n % 3 == 0\n",
    "            sub_n = n // 3\n",
    "            sub_dataset_size = len(base_test[0]) // 3\n",
    "            indices = torch.randperm(sub_dataset_size) # within each bucket, we randomize!\n",
    "            indices = indices[:sub_n]\n",
    "            indices2 = indices + sub_dataset_size\n",
    "            indices3 = indices2 + sub_dataset_size\n",
    "            indices = torch.cat([indices, indices2, indices3])\n",
    "            \n",
    "            return (\n",
    "                (\n",
    "                    [base_test[0][ind] for ind in indices], \n",
    "                    [base_test[1][ind] for ind in indices], \n",
    "                ), \n",
    "                y_base_test[indices], \n",
    "                [\n",
    "                    (\n",
    "                        [sources_test[0][0][ind] for ind in indices],\n",
    "                        [sources_test[0][1][ind] for ind in indices]\n",
    "                    ),\n",
    "                    (\n",
    "                        [sources_test[1][0][ind] for ind in indices],\n",
    "                        [sources_test[1][1][ind] for ind in indices]\n",
    "                    )\n",
    "                ], \n",
    "                y_IIT_test[indices], \n",
    "                intervention_ids_test[indices], \n",
    "            )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def get_IIT_nli_dataset_factual_pairs(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base = dataset.create_factual_pairs()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    return X_base, y_base\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V1(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V1()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources,  y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V2(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V2()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources, y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V1_V2(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V1_V2()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources, y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    assert data_size % 3 == 0\n",
    "    sub_data_size = data_size // 3\n",
    "    V1_dataset = \\\n",
    "        get_IIT_nli_dataset_neghyp_V1(sub_data_size, tokenizer_name, split)\n",
    "    V2_dataset = \\\n",
    "        get_IIT_nli_dataset_neghyp_V2(sub_data_size, tokenizer_name, split)\n",
    "    both_dataset = \\\n",
    "        get_IIT_nli_dataset_neghyp_V1_V2(sub_data_size, tokenizer_name, split)\n",
    "    \n",
    "    X_base = (V1_dataset[0][0] + V2_dataset[0][0] + both_dataset[0][0],\n",
    "     V1_dataset[0][1] + V2_dataset[0][1] + both_dataset[0][1])\n",
    "    y_base = torch.cat((V1_dataset[1],\n",
    "                        V2_dataset[1],\n",
    "                        both_dataset[1]))\n",
    "    \n",
    "    X_sources = [(V1_dataset[2][0][0] + V2_dataset[2][0][0] + both_dataset[2][0][0],\n",
    "    V1_dataset[2][0][1] + V2_dataset[2][0][1] + both_dataset[2][0][1]),\n",
    "    (V1_dataset[2][0][0] + V2_dataset[2][0][0] + both_dataset[2][1][0],\n",
    "    V1_dataset[2][0][1] + V2_dataset[2][0][1] + both_dataset[2][1][1])]\n",
    "    \n",
    "    y_IIT = torch.cat((V1_dataset[3],\n",
    "                        V2_dataset[3],\n",
    "                        both_dataset[3]))\n",
    "    interventions = torch.cat((V1_dataset[4],\n",
    "                        V2_dataset[4],\n",
    "                        both_dataset[4]))\n",
    "    \n",
    "    return X_base, y_base, X_sources, y_IIT, interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d3b3e",
   "metadata": {},
   "source": [
    "### Train Factual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5e43e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "num_layers = 12\n",
    "hidden_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1a2149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training factual model for seed=42\n",
      "Found trained model thus skip: ./saved_models_nli/basemodel-last-12-768-42.bin\n",
      "training factual model for seed=66\n",
      "Found trained model thus skip: ./saved_models_nli/basemodel-last-12-768-66.bin\n",
      "training factual model for seed=77\n",
      "Found trained model thus skip: ./saved_models_nli/basemodel-last-12-768-77.bin\n"
     ]
    }
   ],
   "source": [
    "for seed in {42, 66, 77}:\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    print(f\"training factual model for seed={seed}\")\n",
    "    PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "    if os.path.isfile(PATH):\n",
    "        print(f\"Found trained model thus skip: {PATH}\")\n",
    "        continue\n",
    "    benchmark = IIBenchmarkMoNli(\n",
    "            variable_names=['LEX'],\n",
    "            data_parameters={\n",
    "                'train_size': data_size, 'test_size': test_data_size\n",
    "            },\n",
    "            model_parameters={\n",
    "                'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                'max_length': 128,\n",
    "                'n_classes': 2,\n",
    "                'hidden_dim': 768,\n",
    "                'target_layers' : [],\n",
    "                'target_dims':{\n",
    "                    \"start\" : 0,\n",
    "                    \"end\" : 786,\n",
    "                },\n",
    "                'debug':False, \n",
    "                'device': device\n",
    "            },\n",
    "            training_parameters={\n",
    "                'warm_start': False, 'max_iter': 5, 'batch_size': 32, 'n_iter_no_change': 10000, \n",
    "                'shuffle_train': True, 'eta': 0.00002, 'device': device, 'seed' : seed,\n",
    "            },\n",
    "            seed=seed\n",
    "    )\n",
    "    LIM_bert = benchmark.create_model()\n",
    "    LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "    \n",
    "    X_base_train, y_base_train = get_IIT_nli_dataset_factual_pairs(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name=benchmark.model_parameters[\"weights_name\"]\n",
    "    )\n",
    "    \n",
    "    X_base_test, y_base_test = get_IIT_nli_dataset_factual_pairs(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name=benchmark.model_parameters[\"weights_name\"]\n",
    "    )\n",
    "    \n",
    "    _ = LIM_trainer.fit(\n",
    "        X_base_train, \n",
    "        y_base_train,\n",
    "        save_checkpoint_per_epoch_overwrite=True,\n",
    "        save_checkpoint_prefix=f\"./saved_models_nli/basemodel\"\n",
    "    )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    preds = LIM_trainer.predict(X_base_test)\n",
    "    print(classification_report(y_base_test, preds.cpu()))\n",
    "\n",
    "    torch.save(LIM_bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bb26e",
   "metadata": {},
   "source": [
    "### Train d-IIT Oracle (0, 1) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0837e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 12000\n",
    "test_data_size = 1440\n",
    "num_layers = 12\n",
    "hidden_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8844fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 215.27628363668926"
     ]
    }
   ],
   "source": [
    "oracle_results = []\n",
    "for seed in {42}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_neghyp(\n",
    "        data_size=data_size, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, test_data_size, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_neghyp(\n",
    "        data_size=test_data_size, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {64, 128, 256}\n",
    "        for iit_layer in [10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                0:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}],\n",
    "                1:[{\"layer\":iit_layer, \"start\":hidden_dim_per_concept, \"end\":2*hidden_dim_per_concept}],\n",
    "                2:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept},\n",
    "                   {\"layer\":iit_layer, \"start\":hidden_dim_per_concept, \"end\":2*hidden_dim_per_concept}],\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 10, 'batch_size': 32, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': False, 'eta': 0.004, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                _ = LIM_trainer.fit(\n",
    "                    X_base_train, \n",
    "                    y_base_train, \n",
    "                    iit_data=iit_data,\n",
    "                    intervention_ids_to_coords=intervention_ids_to_coords)\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a483c5c",
   "metadata": {},
   "source": [
    "### Train d-IIT (1, ) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901340bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 441.78408563137054Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 376.6121199131012Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 373.42906749248505Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 351.78610163927087Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 376.34968578815466Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 23.302797263721004Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 14.288326066220184Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 12.274268671637401Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 10.841374445939437Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 13.440728852874613Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 6.0541074307693633Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 4.4996560247382155Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 5 of 5; error is 375.02098411321647Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 8.8817834388464695Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 8.9518477632664145Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 20.904256817884743Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 369.06659013032913Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 39.35332048032433Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 33.20174106117338Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 451.39084303379067/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 8.3058532858267437Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 23.279672581818886Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 5 of 5; error is 59.111437168903655Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 8.8516769353009348Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 16.550571149273316Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 7.3018883567128835Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 7.8539037034788635Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 14.123914585448802"
     ]
    }
   ],
   "source": [
    "control_1_results = []\n",
    "\n",
    "for seed in {42, 66, 77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_neghyp_V2(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, 1000, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_neghyp_V2(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {32, 64, 128}: # {32, 64, 128}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                1:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "            }\n",
    "            for i in [1, 2, 3, 4, 5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 32, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                _ = LIM_trainer.fit(\n",
    "                    X_base_train, \n",
    "                    y_base_train, \n",
    "                    iit_data=iit_data,\n",
    "                    intervention_ids_to_coords=intervention_ids_to_coords)\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a2eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
