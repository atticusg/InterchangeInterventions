{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b777b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_nli\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import random, os\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from LIM_bert import LIMBERTClassifier\n",
    "from ii_benchmark import IIBenchmarkMoNli\n",
    "\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_from_train_nli(iit_nli_dataset, n=1000, control=False):\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = iit_nli_dataset\n",
    "    \n",
    "    if control:\n",
    "        if len(sources_test) == 1:\n",
    "            \n",
    "            \n",
    "            indices = torch.randperm(len(base_test[0])) # within each bucket, we randomize!\n",
    "            indices = indices[:n]\n",
    "            \n",
    "            return (\n",
    "                (\n",
    "                    [base_test[0][ind] for ind in indices], \n",
    "                    [base_test[1][ind] for ind in indices], \n",
    "                ), \n",
    "                y_base_test[indices], \n",
    "                [\n",
    "                    (\n",
    "                        [sources_test[0][0][ind] for ind in indices],\n",
    "                        [sources_test[0][1][ind] for ind in indices]\n",
    "                    )\n",
    "                ], \n",
    "                y_IIT_test[indices], \n",
    "                intervention_ids_test[indices], \n",
    "            )\n",
    "        else:\n",
    "            \n",
    "            assert n % 3 == 0\n",
    "            sub_n = n // 3\n",
    "            sub_dataset_size = len(base_test[0]) // 3\n",
    "            indices = torch.randperm(sub_dataset_size) # within each bucket, we randomize!\n",
    "            indices = indices[:sub_n]\n",
    "            indices2 = indices + sub_dataset_size\n",
    "            indices3 = indices2 + sub_dataset_size\n",
    "            indices = torch.cat([indices, indices2, indices3])\n",
    "            \n",
    "            return (\n",
    "                (\n",
    "                    [base_test[0][ind] for ind in indices], \n",
    "                    [base_test[1][ind] for ind in indices], \n",
    "                ), \n",
    "                y_base_test[indices], \n",
    "                [\n",
    "                    (\n",
    "                        [sources_test[0][0][ind] for ind in indices],\n",
    "                        [sources_test[0][1][ind] for ind in indices]\n",
    "                    ),\n",
    "                    (\n",
    "                        [sources_test[1][0][ind] for ind in indices],\n",
    "                        [sources_test[1][1][ind] for ind in indices]\n",
    "                    )\n",
    "                ], \n",
    "                y_IIT_test[indices], \n",
    "                intervention_ids_test[indices], \n",
    "            )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def get_IIT_nli_dataset_factual_pairs(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base = dataset.create_factual_pairs()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    return X_base, y_base\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V1(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V1()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources,  y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V2(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V2()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources, y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V1_V2(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V1_V2()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources, y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    assert data_size % 3 == 0\n",
    "    sub_data_size = data_size // 3\n",
    "    V1_dataset = \\\n",
    "        get_IIT_nli_dataset_neghyp_V1(sub_data_size, tokenizer_name, split)\n",
    "    V2_dataset = \\\n",
    "        get_IIT_nli_dataset_neghyp_V2(sub_data_size, tokenizer_name, split)\n",
    "    both_dataset = \\\n",
    "        get_IIT_nli_dataset_neghyp_V1_V2(sub_data_size, tokenizer_name, split)\n",
    "    \n",
    "    X_base = (V1_dataset[0][0] + V2_dataset[0][0] + both_dataset[0][0],\n",
    "     V1_dataset[0][1] + V2_dataset[0][1] + both_dataset[0][1])\n",
    "    y_base = torch.cat((V1_dataset[1],\n",
    "                        V2_dataset[1],\n",
    "                        both_dataset[1]))\n",
    "    \n",
    "    X_sources = [(V1_dataset[2][0][0] + V2_dataset[2][0][0] + both_dataset[2][0][0],\n",
    "    V1_dataset[2][0][1] + V2_dataset[2][0][1] + both_dataset[2][0][1]),\n",
    "    (V1_dataset[2][0][0] + V2_dataset[2][0][0] + both_dataset[2][1][0],\n",
    "    V1_dataset[2][0][1] + V2_dataset[2][0][1] + both_dataset[2][1][1])]\n",
    "    \n",
    "    y_IIT = torch.cat((V1_dataset[3],\n",
    "                        V2_dataset[3],\n",
    "                        both_dataset[3]))\n",
    "    interventions = torch.cat((V1_dataset[4],\n",
    "                        V2_dataset[4],\n",
    "                        both_dataset[4]))\n",
    "    \n",
    "    return X_base, y_base, X_sources, y_IIT, interventions\n",
    "\n",
    "def get_IIT_nli_dataset_tokenidentity_V1(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_tokenidentity_V1()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources,  y_IIT, interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d3b3e",
   "metadata": {},
   "source": [
    "### Train Factual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e43e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "num_layers = 12\n",
    "hidden_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in {42, 66, 77}:\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    print(f\"training factual model for seed={seed}\")\n",
    "    PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "    if os.path.isfile(PATH):\n",
    "        print(f\"Found trained model thus skip: {PATH}\")\n",
    "        continue\n",
    "    benchmark = IIBenchmarkMoNli(\n",
    "            variable_names=['LEX'],\n",
    "            data_parameters={\n",
    "                'train_size': data_size, 'test_size': test_data_size\n",
    "            },\n",
    "            model_parameters={\n",
    "                'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                'max_length': 128,\n",
    "                'n_classes': 2,\n",
    "                'hidden_dim': 768,\n",
    "                'target_layers' : [],\n",
    "                'target_dims':{\n",
    "                    \"start\" : 0,\n",
    "                    \"end\" : 786,\n",
    "                },\n",
    "                'debug':False, \n",
    "                'device': device\n",
    "            },\n",
    "            training_parameters={\n",
    "                'warm_start': False, 'max_iter': 5, 'batch_size': 32, 'n_iter_no_change': 10000, \n",
    "                'shuffle_train': True, 'eta': 0.00002, 'device': device, 'seed' : seed,\n",
    "            },\n",
    "            seed=seed\n",
    "    )\n",
    "    LIM_bert = benchmark.create_model()\n",
    "    LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "    \n",
    "    X_base_train, y_base_train = get_IIT_nli_dataset_factual_pairs(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name=benchmark.model_parameters[\"weights_name\"]\n",
    "    )\n",
    "    \n",
    "    X_base_test, y_base_test = get_IIT_nli_dataset_factual_pairs(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name=benchmark.model_parameters[\"weights_name\"]\n",
    "    )\n",
    "    \n",
    "    _ = LIM_trainer.fit(\n",
    "        X_base_train, \n",
    "        y_base_train,\n",
    "        save_checkpoint_per_epoch_overwrite=True,\n",
    "        save_checkpoint_prefix=f\"./saved_models_nli/basemodel\"\n",
    "    )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    preds = LIM_trainer.predict(X_base_test)\n",
    "    print(classification_report(y_base_test, preds.cpu()))\n",
    "\n",
    "    torch.save(LIM_bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bb26e",
   "metadata": {},
   "source": [
    "### Train d-IIT Oracle (0, 1) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 24000\n",
    "test_data_size = 1920\n",
    "num_layers = 12\n",
    "hidden_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_results = []\n",
    "for seed in {77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_neghyp(\n",
    "        data_size=data_size, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, test_data_size, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_neghyp(\n",
    "        data_size=test_data_size, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {64, 128, 256}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                0:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}],\n",
    "                1:[{\"layer\":iit_layer, \"start\":hidden_dim_per_concept, \"end\":2*hidden_dim_per_concept}],\n",
    "                2:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept},\n",
    "                   {\"layer\":iit_layer, \"start\":hidden_dim_per_concept, \"end\":2*hidden_dim_per_concept}],\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': False\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': False, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                _ = LIM_trainer.fit(\n",
    "                    X_base_train, \n",
    "                    y_base_train, \n",
    "                    iit_data=iit_data,\n",
    "                    intervention_ids_to_coords=intervention_ids_to_coords)\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                torch.save(\n",
    "                    LIM_trainer.model.analysis_model.layers[iit_layer_out].weight, \n",
    "                    f\"./saved_models_nli/oracle-rotation_matrix-{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-{seed}.bin\"\n",
    "                )\n",
    "                \n",
    "#                 \n",
    "#                 oracle_results.append(\n",
    "#                     [\n",
    "#                         seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "#                         \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "#                 )\n",
    "#                 oracle_results.append(\n",
    "#                     [\n",
    "#                         seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "#                         \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "#                 )\n",
    "#                 oracle_results.append(\n",
    "#                     [\n",
    "#                         seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "#                         \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "#                 )\n",
    "#                 oracle_results.append(\n",
    "#                     [\n",
    "#                         seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "#                         \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def eigenvalues_to_angles(eigenvalues):\n",
    "    angles = []\n",
    "    for eig in eigenvalues:\n",
    "        angle = np.arctan2(np.imag(eig), np.real(eig))\n",
    "        angles.append(angle)\n",
    "    return angles\n",
    "\n",
    "def to_degree_angles(angles):\n",
    "    degree_angles = set()\n",
    "    for angle in angles:\n",
    "        angle = np.degrees(angle)\n",
    "        degree_angles.add(abs(angle))\n",
    "    return degree_angles\n",
    "\n",
    "R = torch.load(\"./saved_models_nli/rotation_matrix.bin\")\n",
    "w, v = LA.eig(R[:512, :512].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "params = {'mathtext.default': 'regular' }          \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "with plt.rc_context({\n",
    "    'axes.edgecolor':'black', 'xtick.color':'black', \n",
    "    'ytick.color':'black', 'figure.facecolor':'white'\n",
    "}):\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 1.2))\n",
    "\n",
    "    # Create the distribution plot\n",
    "    ax = sns.histplot(\n",
    "        to_degree_angles(eigenvalues_to_angles(w)), \n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Add a title and labels\n",
    "    # ax.set_xlabel(\"Basis Vector Rotation Degree(s)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "\n",
    "    ax.spines[\"top\"].set_linewidth(2)\n",
    "    ax.spines[\"bottom\"].set_linewidth(2)\n",
    "    ax.spines[\"left\"].set_linewidth(2)\n",
    "    ax.spines[\"right\"].set_linewidth(2)\n",
    "    ax.spines[\"top\"].set_linewidth(2)\n",
    "    ax.spines[\"bottom\"].set_linewidth(2)\n",
    "    ax.spines[\"left\"].set_linewidth(2)\n",
    "    ax.spines[\"right\"].set_linewidth(2)\n",
    "    ax.xaxis.grid(color='grey', linestyle='-.', linewidth=1, alpha=0.5)\n",
    "    ax.yaxis.grid(color='grey', linestyle='-.', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    ax.set_facecolor(\"white\")\n",
    "    \n",
    "    plt.legend(labels=['MoNLI'],loc=\"lower right\")\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_df_more = pd.DataFrame(\n",
    "    oracle_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a90add",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_df = pd.read_csv(\"oracle_df.csv\")\n",
    "# oracle_df.to_csv(\"oracle_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c42c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_df = pd.concat([oracle_df, oracle_df_more], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_df[\n",
    "    (oracle_df[\"iit_layer\"] == 9)&\n",
    "    (oracle_df[\"hidden_dim_per_concept\"] == 256)&\n",
    "    (oracle_df[\"epoch\"] == 5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80431092",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=oracle_df[\n",
    "        (oracle_df[\"hidden_dim\"]==768)&\n",
    "        (oracle_df[\"hidden_dim_per_concept\"]==256)&\n",
    "        (oracle_df[\"iit_layer\"]==9)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a483c5c",
   "metadata": {},
   "source": [
    "### Train d-IIT (1, ) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5608e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_1_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901340bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in {77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_neghyp_V2(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, 1000, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_neghyp_V2(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {32, 64, 128}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                1:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': False\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                _ = LIM_trainer.fit(\n",
    "                    X_base_train, \n",
    "                    y_base_train, \n",
    "                    iit_data=iit_data,\n",
    "                    intervention_ids_to_coords=intervention_ids_to_coords)\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                torch.save(\n",
    "                    LIM_trainer.model.analysis_model.layers[iit_layer_out].weight, \n",
    "                    f\"./saved_models_nli/control_1-rotation_matrix-{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-{seed}.bin\"\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_1_df = pd.read_csv(\"control_1_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58eea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=control_1_df[\n",
    "        (control_1_df[\"hidden_dim\"]==768)&\n",
    "        (control_1_df[\"hidden_dim_per_concept\"]==256)&\n",
    "        (control_1_df[\"iit_layer\"]==11)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36107f",
   "metadata": {},
   "source": [
    "### Train d-IIT token identity (1, ) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af61448",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "\n",
    "control_token_1_results = []\n",
    "for seed in {77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, 1000, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {64, 128, 256}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                1:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': False\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                _ = LIM_trainer.fit(\n",
    "                    X_base_train, \n",
    "                    y_base_train, \n",
    "                    iit_data=iit_data,\n",
    "                    intervention_ids_to_coords=intervention_ids_to_coords)\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                torch.save(\n",
    "                    LIM_trainer.model.analysis_model.layers[iit_layer_out].weight, \n",
    "                    f\"./saved_models_nli/control_token_1-rotation_matrix-{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-{seed}.bin\"\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72987429",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81aa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_df = pd.DataFrame(\n",
    "    control_token_1_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_df.to_csv(\"control_token_1_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36db04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_df = pd.read_csv(\"control_token_1_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=control_token_1_df[\n",
    "        (control_token_1_df[\"hidden_dim\"]==768)&\n",
    "        (control_token_1_df[\"hidden_dim_per_concept\"]==256)&\n",
    "        (control_token_1_df[\"iit_layer\"]==11)\n",
    "    ],\n",
    "    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=True,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb87c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_main_result_table(\n",
    "    oracle_df,\n",
    "    control_1_df,\n",
    "    control_token_1_df,\n",
    "    epoch = 5,\n",
    "    hidden_dim = 768,\n",
    "    reduce=max,\n",
    "    eval_setting=\"d-IIT Test\",\n",
    "    round_to=2\n",
    "):\n",
    "    rows = []\n",
    "    for hidden_dim_per_concept in [64, 128, 256]:\n",
    "        row_scores = []\n",
    "        for df in [oracle_df, control_1_df, control_token_1_df]:\n",
    "            selected_scores = []\n",
    "            for iit_layer in [7, 9, 11]:\n",
    "                selected_score = reduce(df[\n",
    "                    (df[\"hidden_dim_per_concept\"]==hidden_dim_per_concept)&\n",
    "                    (df[\"iit_layer\"]==iit_layer)&\n",
    "                    (df[\"hidden_dim\"]==hidden_dim)&\n",
    "                    (df[\"epoch\"]==epoch)&\n",
    "                    (df[\"type\"]==eval_setting)\n",
    "                ][\"f1-score\"].tolist())\n",
    "                selected_scores += [\"%.2f\" % round(selected_score, round_to)]\n",
    "            row_scores.extend(selected_scores)\n",
    "        rows += [row_scores]\n",
    "    df = pd.DataFrame(\n",
    "        rows, \n",
    "        columns=[\n",
    "            \"L7;SCM1\", \"L9;SCM1\", \"L11;SCM1\", \n",
    "            \"L7;SCM2\", \"L9;SCM2\", \"L11;SCM2\", \n",
    "            \"L7;SCM3\", \"L9;SCM3\", \"L11;SCM3\"\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_main_result_table(\n",
    "    oracle_df,\n",
    "    control_1_df,\n",
    "    control_token_1_df\n",
    ").to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [768]\n",
    "SCMs = [\"oracle\", \"control_1_df\", \"control_token_1_df\"]\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for SCM in SCMs:\n",
    "\n",
    "        if SCM == \"oracle\":\n",
    "            df = oracle_df\n",
    "        elif SCM == \"control_1_df\":\n",
    "            df = control_1_df\n",
    "        elif SCM == \"control_token_1_df\":\n",
    "            df = control_token_1_df\n",
    "\n",
    "        plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "        font = {'family' : 'DejaVu Serif',\n",
    "                'size'   : 20}\n",
    "        plt.rc('font', **font)\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(14,10))\n",
    "\n",
    "        hidden_dim_per_concepts = [64, 128, 256]\n",
    "        iit_layers = [7,9,11]\n",
    "        ylabels = [f\"k=64\", f\"k=128\", f\"k=256\"]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                hidden_dim_per_concept = hidden_dim_per_concepts[i]\n",
    "                iit_layer = iit_layers[j]\n",
    "                df_toplot = df[(df[\"hidden_dim_per_concept\"]==hidden_dim_per_concept)&\n",
    "                   (df[\"iit_layer\"]==iit_layer)&\n",
    "                   (df[\"hidden_dim\"]==hidden_dim)&\n",
    "                   ((df[\"type\"]==\"Factual Train\")|\n",
    "                    (df[\"type\"]==\"d-IIT Train\")\n",
    "                    |(df[\"type\"]==\"Factual Test\")\n",
    "                    |(df[\"type\"]==\"d-IIT Test\"))\n",
    "                ]\n",
    "                sns.lineplot(\n",
    "                    ax=axes[i,j],\n",
    "                    data=df_toplot,\n",
    "                    x=\"epoch\", y=\"f1-score\", hue=\"type\", style=\"type\",\n",
    "                    dashes=False, markers=['o', 's', '^', 'D'], markersize=12, legend=False,\n",
    "                    alpha=0.8\n",
    "                )\n",
    "                axes[i,j].set_ylim(0.5, 1.1)\n",
    "                if i == 2:\n",
    "                    axes[i,j].set(xlabel=\"Epoch\" if j == 1 else None, xticks=[1,2,3,4,5], xticklabels=[1,2,3,4,5])\n",
    "                else:\n",
    "                    axes[i,j].set(xlabel=None, xticks=[1,2,3,4,5], xticklabels=[])\n",
    "                if j == 0:\n",
    "                    axes[i,j].set(ylabel=ylabels[i])\n",
    "                else:\n",
    "                    axes[i,j].set(ylabel=None, yticklabels=[])\n",
    "\n",
    "        axes[0,0].set_title(\"L7\")\n",
    "        axes[0,1].set_title(\"L9\")\n",
    "        axes[0,2].set_title(\"L11\")\n",
    "\n",
    "        plt.legend(loc='lower right', labels=['Task Acc. (Train)', 'Int. Acc. (Train)', \n",
    "                                               'Task Acc. (Test)', 'Int. Acc. (Test)'], fontsize=14)\n",
    "\n",
    "        # plt.show()\n",
    "        plt.savefig(f\"./fig/MoNLI-{hidden_dim}-{SCM}.png\",dpi=200, bbox_inches='tight')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "\n",
    "control_token_1_control_results = []\n",
    "for seed in {66, 77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=data_size, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, test_data_size, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=test_data_size, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {64, 128, 256}\n",
    "        for start_index in {0, 64, 128, 256, 512}:\n",
    "            for iit_layer in [0,1,2,3,4,5,6,7,8,9,10,11]: # [6, 8, 10]\n",
    "                print(f\"searching with layer = {iit_layer}, start_index = {start_index}\")\n",
    "                intervention_ids_to_coords = {\n",
    "                    0:[{\"layer\":iit_layer, \"start\":0+start_index, \"end\":hidden_dim_per_concept+start_index}],\n",
    "                    1:[{\"layer\":iit_layer, \"start\":hidden_dim_per_concept+start_index, \"end\":2*hidden_dim_per_concept+start_index}],\n",
    "                    2:[{\"layer\":iit_layer, \"start\":0+start_index, \"end\":hidden_dim_per_concept+start_index},\n",
    "                       {\"layer\":iit_layer, \"start\":hidden_dim_per_concept+start_index, \"end\":2*hidden_dim_per_concept+start_index}],\n",
    "                }\n",
    "                for i in [5]: # 1, 2, 3, 4, 5\n",
    "                    torch.cuda.empty_cache()\n",
    "                    benchmark = IIBenchmarkMoNli(\n",
    "                            variable_names=['LEX'],\n",
    "                            data_parameters={\n",
    "                                'train_size': data_size, 'test_size': test_data_size\n",
    "                            },\n",
    "                            model_parameters={\n",
    "                                'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                                'max_length': 128,\n",
    "                                'n_classes': 2,\n",
    "                                'hidden_dim': 768,\n",
    "                                'target_layers' : [iit_layer],\n",
    "                                'target_dims':{\n",
    "                                    \"start\" : 0,\n",
    "                                    \"end\" : 786,\n",
    "                                },\n",
    "                                'debug':False, \n",
    "                                'device': device,\n",
    "                                'static_search': True\n",
    "                            },\n",
    "                            training_parameters={\n",
    "                                'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                                'shuffle_train': False, 'eta': 0.002, 'device': device\n",
    "                            },\n",
    "                            seed=seed\n",
    "                    )\n",
    "                    LIM_bert = benchmark.create_model()\n",
    "                    new_state_dict = {}\n",
    "                    ORACLE_PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                    for k, v in torch.load(ORACLE_PATH).items():\n",
    "                        if \"analysis_model\" not in k:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                                new_state_dict[k] = v\n",
    "                            else:\n",
    "                                new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                                k_list = k.split(\".\")\n",
    "                                k_list[2] = str(new_layer_number)\n",
    "                                new_k = \".\".join(k_list)\n",
    "                                new_state_dict[new_k] = v\n",
    "                    LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                    LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                    LIM_trainer.model.set_analysis_mode(True)\n",
    "\n",
    "                    # train data eval\n",
    "                    base_preds_train = LIM_trainer.predict(\n",
    "                        base_test_train\n",
    "                    )\n",
    "                    IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                        base_test_train, sources_test_train, \n",
    "                        intervention_ids_test_train, \n",
    "                        intervention_ids_to_coords\n",
    "                    )\n",
    "                    r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                    r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                    # test data eval\n",
    "                    base_preds = LIM_trainer.predict(\n",
    "                        base_test\n",
    "                    )\n",
    "                    IIT_preds = LIM_trainer.iit_predict(\n",
    "                        base_test, sources_test, \n",
    "                        intervention_ids_test, \n",
    "                        intervention_ids_to_coords\n",
    "                    )\n",
    "                    r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                    r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "\n",
    "                    iit_layer_out = iit_layer + 1\n",
    "                    control_token_1_control_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, start_index, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_token_1_control_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, start_index, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_token_1_control_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, start_index, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )\n",
    "                    control_token_1_control_results.append(\n",
    "                        [\n",
    "                            seed, hidden_dim, start_index, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                            \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c995c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_control_df = pd.DataFrame(\n",
    "    control_token_1_control_results,\n",
    "    columns =['seed', 'hidden_dim', 'start_index', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(control_token_1_control_df[\n",
    "    (control_token_1_control_df[\"type\"]==\"d-IIT Train\")&\n",
    "    (control_token_1_control_df[\"iit_layer\"]==7)\n",
    "][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(control_token_1_control_df[\n",
    "    (control_token_1_control_df[\"type\"]==\"d-IIT Train\")&\n",
    "    (control_token_1_control_df[\"iit_layer\"]==9)\n",
    "][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e08d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(control_token_1_control_df[\n",
    "    (control_token_1_control_df[\"type\"]==\"d-IIT Train\")&\n",
    "    (control_token_1_control_df[\"iit_layer\"]==11)\n",
    "][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59f99a",
   "metadata": {},
   "source": [
    "### Double Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d316c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "\n",
    "control_token_1_double_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in {42, 66, 77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, 1000, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {32,}: # {64, 128, 256}\n",
    "        for iit_layer in [8]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                1:[{\"layer\":iit_layer+1, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': True\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                iit_layer = iit_layer + 1\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/first_stage.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True, layers=[iit_layer])\n",
    "\n",
    "                _ = LIM_trainer.fit(\n",
    "                    X_base_train, \n",
    "                    y_base_train, \n",
    "                    iit_data=iit_data,\n",
    "                    intervention_ids_to_coords=intervention_ids_to_coords)\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                iit_layer_out = iit_layer\n",
    "                control_token_1_double_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_double_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_double_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_double_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_double_df = pd.DataFrame(\n",
    "    control_token_1_double_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(control_token_1_double_df[\n",
    "    (control_token_1_double_df[\"type\"] == \"d-IIT Train\")&\n",
    "    (control_token_1_double_df[\"hidden_dim_per_concept\"] == 32)\n",
    "][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_double_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbae2e",
   "metadata": {},
   "source": [
    "### Evaluate closest localist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 24000\n",
    "test_data_size = 1920\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "oracle_results = []\n",
    "for seed in {77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)   \n",
    "    train_datasetIIT = get_IIT_nli_dataset_neghyp(\n",
    "        data_size=data_size, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, test_data_size, control=True\n",
    "        )\n",
    "\n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_neghyp(\n",
    "        data_size=test_data_size, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    ) \n",
    "    for hidden_dim_per_concept in {256}: # {64, 128, 256}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                0:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}],\n",
    "                1:[{\"layer\":iit_layer, \"start\":hidden_dim_per_concept, \"end\":2*hidden_dim_per_concept}],\n",
    "                2:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept},\n",
    "                   {\"layer\":iit_layer, \"start\":hidden_dim_per_concept, \"end\":2*hidden_dim_per_concept}],\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': False\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': False, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-{i}-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(torch.load(ORACLE_PATH), strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                iit_layer_out = iit_layer + 1\n",
    "                \n",
    "                # load rotation matrix\n",
    "                R = torch.load(\n",
    "                    f\"./saved_models_nli/oracle-rotation_matrix-{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-{seed}.bin\"\n",
    "                )\n",
    "                abs_R = torch.abs(R)\n",
    "                sign = -1 * torch.ones_like(R) * (R<0) + torch.ones_like(R) * (R>0)\n",
    "                snapped_R = torch.zeros_like(R)\n",
    "\n",
    "                local_max_pos = []\n",
    "                for i in range(R.shape[0]):\n",
    "                    local_max_pos += [(abs_R==torch.max(abs_R)).nonzero()]\n",
    "                    select_row = local_max_pos[-1][0,0].tolist()\n",
    "                    select_col = local_max_pos[-1][0,1].tolist()\n",
    "                    abs_R[select_row, :] = 0.\n",
    "                    abs_R[:, select_col] = 0.\n",
    "                for pos in local_max_pos:\n",
    "                    snapped_R[pos[0][0], pos[0][1]] = 1.\n",
    "\n",
    "                snapped_R *= sign\n",
    "                snapped_R_T = torch.transpose(snapped_R, 0, 1)\n",
    "                snapped_R = snapped_R.to(LIM_trainer.model.analysis_model.layers[iit_layer_out].weight.device)\n",
    "                LIM_trainer.model.analysis_model.layers[iit_layer_out].weight = snapped_R\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                \n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                oracle_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                \n",
    "oracle_df = pd.DataFrame(\n",
    "    oracle_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7dbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_df[\n",
    "    oracle_df[\"type\"]==\"d-IIT Train\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61349c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_1_results = []\n",
    "for seed in {77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_neghyp_V2(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, 1000, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_neghyp_V2(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {32, 64, 128}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                1:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': False\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                \n",
    "                # load rotation matrix\n",
    "                R = torch.load(\n",
    "                    f\"./saved_models_nli/control_1-rotation_matrix-{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-{seed}.bin\"\n",
    "                )\n",
    "                abs_R = torch.abs(R)\n",
    "                sign = -1 * torch.ones_like(R) * (R<0) + torch.ones_like(R) * (R>0)\n",
    "                snapped_R = torch.zeros_like(R)\n",
    "\n",
    "                local_max_pos = []\n",
    "                for i in range(R.shape[0]):\n",
    "                    local_max_pos += [(abs_R==torch.max(abs_R)).nonzero()]\n",
    "                    select_row = local_max_pos[-1][0,0].tolist()\n",
    "                    select_col = local_max_pos[-1][0,1].tolist()\n",
    "                    abs_R[select_row, :] = 0.\n",
    "                    abs_R[:, select_col] = 0.\n",
    "                for pos in local_max_pos:\n",
    "                    snapped_R[pos[0][0], pos[0][1]] = 1.\n",
    "\n",
    "                snapped_R *= sign\n",
    "                snapped_R_T = torch.transpose(snapped_R, 0, 1)\n",
    "                snapped_R = snapped_R.to(LIM_trainer.model.analysis_model.layers[iit_layer_out].weight.device)\n",
    "                LIM_trainer.model.analysis_model.layers[iit_layer_out].weight = snapped_R\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_1_df = pd.DataFrame(\n",
    "    control_1_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")\n",
    "control_1_df[\n",
    "    control_1_df[\"type\"]==\"d-IIT Train\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "num_layers = 12\n",
    "hidden_dim = 768\n",
    "\n",
    "control_token_1_results = []\n",
    "for seed in {77}: # {42, 66, 77}\n",
    "    utils.fix_random_seeds(seed=seed)\n",
    "    train_datasetIIT = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=10000, \n",
    "        split=\"train\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )\n",
    "    X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "    iit_data = tuple(train_datasetIIT[2:])\n",
    "\n",
    "    base_test_train, y_base_test_train, sources_test_train, y_IIT_test_train, intervention_ids_test_train = \\\n",
    "        get_eval_from_train_nli(\n",
    "            train_datasetIIT, 1000, control=True\n",
    "        )\n",
    "    \n",
    "    base_test, y_base_test, sources_test, y_IIT_test, intervention_ids_test = get_IIT_nli_dataset_tokenidentity_V1(\n",
    "        data_size=1000, \n",
    "        split=\"test\",\n",
    "        tokenizer_name='ishan/bert-base-uncased-mnli'\n",
    "    )    \n",
    "    for hidden_dim_per_concept in {256}: # {64, 128, 256}\n",
    "        for iit_layer in [6, 8, 10]: # [6, 8, 10]\n",
    "            intervention_ids_to_coords = {\n",
    "                1:[{\"layer\":iit_layer, \"start\":0, \"end\":hidden_dim_per_concept}]\n",
    "            }\n",
    "            for i in [5]: # 1, 2, 3, 4, 5\n",
    "                torch.cuda.empty_cache()\n",
    "                benchmark = IIBenchmarkMoNli(\n",
    "                        variable_names=['LEX'],\n",
    "                        data_parameters={\n",
    "                            'train_size': data_size, 'test_size': test_data_size\n",
    "                        },\n",
    "                        model_parameters={\n",
    "                            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "                            'max_length': 128,\n",
    "                            'n_classes': 2,\n",
    "                            'hidden_dim': 768,\n",
    "                            'target_layers' : [iit_layer],\n",
    "                            'target_dims':{\n",
    "                                \"start\" : 0,\n",
    "                                \"end\" : 786,\n",
    "                            },\n",
    "                            'debug':False, \n",
    "                            'device': device,\n",
    "                            'static_search': False,\n",
    "                            'nested_disentangle_inplace': False\n",
    "                        },\n",
    "                        training_parameters={\n",
    "                            'warm_start': False, 'max_iter': 5, 'batch_size': 64, 'n_iter_no_change': 10000, \n",
    "                            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "                        },\n",
    "                        seed=seed\n",
    "                )\n",
    "                LIM_bert = benchmark.create_model()\n",
    "                new_state_dict = {}\n",
    "                ORACLE_PATH = f\"./saved_models_nli/basemodel-last-{num_layers}-{hidden_dim}-{seed}.bin\"\n",
    "                for k, v in torch.load(ORACLE_PATH).items():\n",
    "                    if \"analysis_model\" not in k:\n",
    "                        new_state_dict[k] = v\n",
    "                    else:\n",
    "                        if int(k.split(\".\")[2]) <= iit_layer:\n",
    "                            new_state_dict[k] = v\n",
    "                        else:\n",
    "                            new_layer_number = int(k.split(\".\")[2]) + 2\n",
    "                            k_list = k.split(\".\")\n",
    "                            k_list[2] = str(new_layer_number)\n",
    "                            new_k = \".\".join(k_list)\n",
    "                            new_state_dict[new_k] = v\n",
    "                LIM_bert.load_state_dict(new_state_dict, strict=False)\n",
    "                LIM_trainer = benchmark.create_classifier(LIM_bert)\n",
    "                LIM_trainer.model.set_analysis_mode(True)\n",
    "                \n",
    "                iit_layer_out = iit_layer + 1\n",
    "                \n",
    "                # load rotation matrix\n",
    "                R = torch.load(\n",
    "                    f\"./saved_models_nli/control_token_1-rotation_matrix-{iit_layer_out}-{hidden_dim}-{hidden_dim_per_concept}-{seed}.bin\"\n",
    "                )\n",
    "                abs_R = torch.abs(R)\n",
    "                sign = -1 * torch.ones_like(R) * (R<0) + torch.ones_like(R) * (R>0)\n",
    "                snapped_R = torch.zeros_like(R)\n",
    "\n",
    "                local_max_pos = []\n",
    "                for i in range(R.shape[0]):\n",
    "                    local_max_pos += [(abs_R==torch.max(abs_R)).nonzero()]\n",
    "                    select_row = local_max_pos[-1][0,0].tolist()\n",
    "                    select_col = local_max_pos[-1][0,1].tolist()\n",
    "                    abs_R[select_row, :] = 0.\n",
    "                    abs_R[:, select_col] = 0.\n",
    "                for pos in local_max_pos:\n",
    "                    snapped_R[pos[0][0], pos[0][1]] = 1.\n",
    "\n",
    "                snapped_R *= sign\n",
    "                snapped_R_T = torch.transpose(snapped_R, 0, 1)\n",
    "                snapped_R = snapped_R.to(LIM_trainer.model.analysis_model.layers[iit_layer_out].weight.device)\n",
    "                LIM_trainer.model.analysis_model.layers[iit_layer_out].weight = snapped_R\n",
    "                \n",
    "                # train data eval\n",
    "                base_preds_train = LIM_trainer.predict(\n",
    "                    base_test_train\n",
    "                )\n",
    "                IIT_preds_train = LIM_trainer.iit_predict(\n",
    "                    base_test_train, sources_test_train, \n",
    "                    intervention_ids_test_train, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1_train = classification_report(y_base_test_train, base_preds_train.cpu(), output_dict=True)\n",
    "                r2_train = classification_report(y_IIT_test_train, IIT_preds_train.cpu(), output_dict=True)\n",
    "\n",
    "                # test data eval\n",
    "                base_preds = LIM_trainer.predict(\n",
    "                    base_test\n",
    "                )\n",
    "                IIT_preds = LIM_trainer.iit_predict(\n",
    "                    base_test, sources_test, \n",
    "                    intervention_ids_test, \n",
    "                    intervention_ids_to_coords\n",
    "                )\n",
    "                r1 = classification_report(y_base_test, base_preds.cpu(), output_dict=True)\n",
    "                r2 = classification_report(y_IIT_test, IIT_preds.cpu(), output_dict=True)\n",
    "                \n",
    "                \n",
    "\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Train\", r1_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Train\", r2_train[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"Factual Test\", r1[\"weighted avg\"][\"f1-score\"]]\n",
    "                )\n",
    "                control_token_1_results.append(\n",
    "                    [\n",
    "                        seed, hidden_dim, hidden_dim_per_concept, iit_layer_out, i, \n",
    "                        \"d-IIT Test\", r2[\"weighted avg\"][\"f1-score\"]]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57514ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_token_1_df = pd.DataFrame(\n",
    "    control_token_1_results,\n",
    "    columns =['seed', 'hidden_dim', 'hidden_dim_per_concept', 'iit_layer', 'epoch', \n",
    "              'type', 'f1-score']\n",
    ")\n",
    "control_token_1_df[\n",
    "    control_token_1_df[\"type\"]==\"d-IIT Train\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fbbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
