{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd00258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_nli\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from LIM_bert import LIMBERTClassifier\n",
    "from ii_benchmark import IIBenchmarkMoNli\n",
    "\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f78c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "data_size = 10000\n",
    "test_data_size = 1000\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21240208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IIT_nli_dataset_factual_pairs(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base = dataset.create_factual_pairs()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    return X_base, y_base\n",
    "\n",
    "def get_IIT_nli_dataset_neghyp_V2(\n",
    "    data_size,\n",
    "    tokenizer_name,\n",
    "    split=\"train\",\n",
    "):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def encoding(X):\n",
    "        if X[0][-1] != \".\":\n",
    "            input = [\". \".join(X)]\n",
    "        else:\n",
    "            input = [\" \".join(X)]\n",
    "        data = bert_tokenizer.batch_encode_plus(\n",
    "                input,\n",
    "                max_length=128,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        return (indices, mask)\n",
    "    \n",
    "    dataset = dataset_nli.IIT_MoNLIDataset(\n",
    "        embed_func=encoding,\n",
    "        suffix=split,\n",
    "        size=data_size)\n",
    "    \n",
    "    X_base, y_base, X_sources,  y_IIT, interventions = dataset.create_neghyp_V2()\n",
    "    y_base = torch.tensor(y_base)\n",
    "    y_IIT = torch.tensor(y_IIT)\n",
    "    interventions = torch.tensor(interventions)\n",
    "    return X_base, y_base, X_sources,  y_IIT, interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7f7f4",
   "metadata": {},
   "source": [
    "factual model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce094ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "benchmark = IIBenchmarkMoNli(\n",
    "        variable_names=['LEX'],\n",
    "        data_parameters={\n",
    "            'train_size': data_size, 'test_size': test_data_size\n",
    "        },\n",
    "        model_parameters={\n",
    "            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "            'max_length': 128,\n",
    "            'n_classes': 2,\n",
    "            'hidden_dim': 768,\n",
    "            'target_layers' : [10],\n",
    "            'target_dims':{\n",
    "                \"start\" : 0,\n",
    "                \"end\" : 786,\n",
    "            },\n",
    "            'debug':False, \n",
    "            'device': device\n",
    "        },\n",
    "        training_parameters={\n",
    "            'warm_start': False, 'max_iter': 3, 'batch_size': 32, 'n_iter_no_change': 10000, \n",
    "            'shuffle_train': True, 'eta': 0.00002, 'device': device\n",
    "        },\n",
    "        seed=seed\n",
    ")\n",
    "LIM_bert = benchmark.create_model()\n",
    "LIM_trainer = benchmark.create_classifier(LIM_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "336e9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds(seed=42)\n",
    "train_datasetIIT = get_IIT_nli_dataset_neghyp_V2(\n",
    "    data_size=10000, \n",
    "    split=\"train\",\n",
    "    tokenizer_name=benchmark.model_parameters[\"weights_name\"]\n",
    ")\n",
    "X_base_train, y_base_train = train_datasetIIT[0:2]\n",
    "iit_data = tuple(train_datasetIIT[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "485eb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasetIIT = get_IIT_nli_dataset_neghyp_V2(\n",
    "    data_size=1000, \n",
    "    split=\"test\",\n",
    "    tokenizer_name=benchmark.model_parameters[\"weights_name\"]\n",
    ")\n",
    "X_base_test, y_base_test = test_datasetIIT[0:2]\n",
    "iit_data_test = tuple(test_datasetIIT[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3c5dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 3 of 3; error is 1.2753275868890341"
     ]
    }
   ],
   "source": [
    "_ = LIM_trainer.fit(\n",
    "    X_base_train, \n",
    "    y_base_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "677471b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       500\n",
      "           1       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "preds = LIM_trainer.predict(X_base_test)\n",
    "print(classification_report(y_base_test, preds.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0153e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"./saved_models_nli/basemodel-last-bert-{seed}.bin\"\n",
    "torch.save(LIM_bert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f84c7",
   "metadata": {},
   "source": [
    "d-iit training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bacd05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ishan/bert-base-uncased-mnli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "benchmark = IIBenchmarkMoNli(\n",
    "        variable_names=['LEX'],\n",
    "        data_parameters={\n",
    "            'train_size': data_size, 'test_size': test_data_size\n",
    "        },\n",
    "        model_parameters={\n",
    "            'weights_name': 'ishan/bert-base-uncased-mnli',\n",
    "            'max_length': 128,\n",
    "            'n_classes': 2,\n",
    "            'hidden_dim': 768,\n",
    "            'target_layers' : [10],\n",
    "            'target_dims':{\n",
    "                \"start\" : 0,\n",
    "                \"end\" : 786,\n",
    "            },\n",
    "            'debug':False, \n",
    "            'device': device\n",
    "        },\n",
    "        training_parameters={\n",
    "            'warm_start': False, 'max_iter': 10, 'batch_size': 32, 'n_iter_no_change': 10000, \n",
    "            'shuffle_train': True, 'eta': 0.002, 'device': device\n",
    "        },\n",
    "        seed=seed\n",
    ")\n",
    "LIM_bert = benchmark.create_model()\n",
    "ORACLE_PATH = f\"./saved_models_nli/basemodel-last-bert-{seed}.bin\"\n",
    "LIM_bert.load_state_dict(torch.load(ORACLE_PATH))\n",
    "LIM_trainer = benchmark.create_classifier(LIM_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d01cdfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       500\n",
      "           1       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LIM_trainer.model.set_analysis_mode(True)\n",
    "torch.cuda.empty_cache()\n",
    "preds = LIM_trainer.predict(X_base_test)\n",
    "print(classification_report(y_base_test, preds.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_ids_to_coords = {\n",
    "    1:[{\"layer\":10, \"start\":0, \"end\":64}]\n",
    "}\n",
    "_ = LIM_trainer.fit(\n",
    "    X_base_train, \n",
    "    y_base_train, \n",
    "    iit_data=iit_data,\n",
    "    intervention_ids_to_coords=intervention_ids_to_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fb349c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_test, y_IIT_test, intervention_ids_test = iit_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2da37414",
   "metadata": {},
   "outputs": [],
   "source": [
    "IIT_preds_test = LIM_trainer.iit_predict(\n",
    "    X_base_test, sources_test, \n",
    "    intervention_ids_test, \n",
    "    intervention_ids_to_coords, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8f89c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       500\n",
      "           1       0.94      0.95      0.94       500\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_IIT_test, IIT_preds_test.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe5f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
